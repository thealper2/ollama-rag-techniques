{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87463db2-7c2a-437d-b38a-5ccdff6ec74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27866640-686a-4a38-84fc-b1cdc68244d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCH:\n",
    "    \"\"\"\n",
    "    A RAG pipeline with Contextual Chunk Headers (CCH) implementation using Ollama.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"llama3.2:3b\"):\n",
    "        \"\"\"\n",
    "        Initialize the CCH RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the Ollama model to use (default: \"llama3.2:3b\")\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.llm = None\n",
    "        self.embeddings = None\n",
    "        self.vectorstore = None\n",
    "        self.documents = []\n",
    "\n",
    "    def setup_environment(self) -> None:\n",
    "        \"\"\"\n",
    "        Set up the environment including LLM and embeddings.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize Ollama LLM\n",
    "            self.llm = ChatOllama(model=self.model_name)\n",
    "\n",
    "            # Initialize Ollama Embeddings\n",
    "            self.embeddings = OllamaEmbeddings(model=self.model_name)\n",
    "\n",
    "            print(\"Environment setup complete with model:\", self.model_name)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up environment: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_and_extract_pdf(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load and extract text from a PDF file with section headers.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the PDF file\n",
    "\n",
    "        Returns:\n",
    "            List of Documents with extracted text and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"PDF file not found at {file_path}\")\n",
    "\n",
    "            # Load PDF with PYPDFLoader\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            self.documents = loader.load()\n",
    "\n",
    "            print(f\"Successfully loaded {len(self.documents)} pages from PDF\")\n",
    "            return self.documents\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading PDF: {e}\")\n",
    "            raise\n",
    "\n",
    "    def identify_section_headers(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Identify section headers in documents using LLM.\n",
    "        Adds header information to document metadata.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): List of document to process\n",
    "\n",
    "        Returns:\n",
    "            List of documents with header metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prompt to identify headers\n",
    "            header_prompt = \"\"\"Generate a concise and informative title for the given text.\n",
    "            Return only the header text if found, otherwise return 'None':\n",
    "            \n",
    "            {page_content}\"\"\"\n",
    "\n",
    "            prompt_template = ChatPromptTemplate.from_template(header_prompt)\n",
    "            header_chain = prompt_template | self.llm | StrOutputParser()\n",
    "\n",
    "            processed_docs = []\n",
    "            for doc in documents:\n",
    "                # Get header from LLM\n",
    "                header = header_chain.invoke({\"page_content\": doc.page_content})\n",
    "\n",
    "                # Add header to metadata\n",
    "                metadata = doc.metadata.copy()\n",
    "                metadata[\"header\"] = header if header.lower() != \"none\" else None\n",
    "                processed_docs.append(\n",
    "                    Document(page_content=doc.page_content, metadata=metadata)\n",
    "                )\n",
    "\n",
    "            print(\"Section headers identified for documents.\")\n",
    "            return processed_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error identify section headers: {e}\")\n",
    "            raise\n",
    "\n",
    "    def chunk_with_contextual_headers(\n",
    "        self,\n",
    "        documents: List[Document],\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split documents into chunks while preserving contextual headers.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): List of documents to chunk\n",
    "            chunk_size (int): Size of each chunk in characters (default: 1000)\n",
    "            chunk_overlap (int): Overlap between chunks (default: 200)\n",
    "\n",
    "        Returns:\n",
    "            List of chunked documents with header context\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First split by sections (headers)\n",
    "            section_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size * 2,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \"  \", \" \"],\n",
    "            )\n",
    "\n",
    "            # Then split into smaller chunks with semantic awareness\n",
    "            semantic_splitter = SemanticChunker(self.embeddings)\n",
    "\n",
    "            chunked_docs = []\n",
    "            for doc in documents:\n",
    "                # Get header from metadata\n",
    "                header = doc.metadata.get(\"header\", \"\")\n",
    "\n",
    "                # Split by sections first\n",
    "                section_chunks = section_splitter.split_documents([doc])\n",
    "\n",
    "                for section in section_chunks:\n",
    "                    # Split sections into smaller semantic chunks\n",
    "                    semantic_chunks = semantic_splitter.split_documents([section])\n",
    "\n",
    "                    # Add header context to each chunk\n",
    "                    for chunk in semantic_chunks:\n",
    "                        chunk_metadata = chunk.metadata.copy()\n",
    "                        chunk_metadata[\"header\"] = header\n",
    "                        chunked_docs.append(\n",
    "                            Document(\n",
    "                                page_content=chunk.page_content, metadata=chunk_metadata\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            print(f\"Created {len(chunked_docs)} chunks with contextual headers.\")\n",
    "            return chunked_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error chunking documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_vector_store(self, chunks: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Create a vector store from document chunks with embeddings.\n",
    "\n",
    "        Args:\n",
    "            chunks (List[Document]): List of document chunks to index\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create FAISS vector store with Ollama embeddings\n",
    "            self.vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
    "            print(\"Vector store created with\", len(chunks), \"chunks.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_search(self, query: str, k: int = 4) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Perform semantic search on the vector store and return results in a formatted way.\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query\n",
    "            k (int): Number of results to return\n",
    "\n",
    "        Return:\n",
    "            List of dictionaries containing header and text for each chunk\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.vectorstore:\n",
    "                raise ValueError(\"Vector store not initialized.\")\n",
    "\n",
    "            # Perform similarity search\n",
    "            results = self.vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "            # Format results\n",
    "            formatted_results = []\n",
    "            for doc in results:\n",
    "                formatted_results.append(\n",
    "                    {\n",
    "                        \"header\": doc.metadata.get(\"header\", \"No header\"),\n",
    "                        \"text\": doc.page_content,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            return formatted_results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error performing semantic search: {e}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using the LLM with retrieved context.\n",
    "\n",
    "        Args:\n",
    "            query (str): User query\n",
    "            context (List[Dict[str, str]]): Retrieved relevant documents in formatted dictionary\n",
    "\n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare context string\n",
    "            context_str = \"\\n\\n---\\n\\n\".join(\n",
    "                [\n",
    "                    f\"Header: {chunk['header']}\\nContent: {chunk['text']}\"\n",
    "                    for chunk in context\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create RAG prompt\n",
    "            prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful assistant that answers questions based on the provided context.\n",
    "            The context includes section headers that provide important structure.\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Provide a detailed answer based on the context. If the answer isn't in the context, \n",
    "            say you don't know. Pay attention to section headers as they indicate important topics.\n",
    "            \"\"\")\n",
    "\n",
    "            # Create and run chain\n",
    "            chain = prompt | self.llm | StrOutputParser()\n",
    "            response = chain.invoke({\"question\": query, \"context\": context_str})\n",
    "\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Complete RAG pipeline for a query: search + generation.\n",
    "        Prints the query and retrieved chunks in the requested format.\n",
    "\n",
    "        Args:\n",
    "            query (str): User query\n",
    "\n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Semantic search\n",
    "            relevant_chunks = self.semantic_search(query)\n",
    "\n",
    "            # Print query and chunks\n",
    "            print(\"\\nQuery:\", query)\n",
    "            for i, chunk in enumerate(relevant_chunks):\n",
    "                print(f\"\\nHeader {i + 1}: {chunk['header']}\")\n",
    "                print(f\"Content:\\n{chunk['text']}\\n\")\n",
    "\n",
    "            # Step 2: Generate response\n",
    "            response = self.generate_response(query, relevant_chunks)\n",
    "\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error running query: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1603964-eca5-4fda-af53-bbf3d5fb2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./dataset/health supplements/1. dietary supplements - for whom.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c1643-d7b3-43fe-ab20-cfba17cb1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = CCH(model_name=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8d28f-7477-4669-b7e4-7dd0695a652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c6846-9f48-4cd3-ac57-5dc8f4c86527",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = rag.load_and_extract_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793ca97-8920-4fed-9a4e-34be46672136",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_headers = rag.identify_section_headers(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822f857-d0dc-441b-8b0b-7e7f6a758c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = rag.chunk_with_contextual_headers(documents_with_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ca554-fda0-4d4e-9c36-615ca246da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.create_vector_store(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ed62d-28d3-42eb-aa9f-ad16d5801ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic discussed in section 3?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa70f1-08f0-406c-96d8-fd24c5dbb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag.run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5111a-f7d4-409e-bfc1-ec877b5777b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Response to '{query}':\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b9676-3258-423d-8d99-d967d957a9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
