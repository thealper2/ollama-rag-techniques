{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db49876-376a-4050-8299-c17f1df0c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import fitz\n",
    "import numpy as np\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390d963-73fa-45bb-abe4-0eb3c0b93d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFTextExtractor:\n",
    "    \"\"\"\n",
    "    A class to handle PDF text extraction using PyMuPDF.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the PDFTextExtractor with a file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the PDF file\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def extract_text(self) -> str:\n",
    "        \"\"\"\n",
    "        Extract all text from the PDF file.\n",
    "\n",
    "        Returns:\n",
    "            str: Extracted text from the PDF\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the PDF file doesn't exist\n",
    "            Exception: For other extraction errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            full_text = []\n",
    "            with fitz.open(self.file_path) as doc:\n",
    "                for page in doc:\n",
    "                    full_text.append(page.get_text())\n",
    "            return \"\\n\".join(full_text)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"The file {self.file_path} was not found.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred while extracting text: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f4f9c-d52f-4d4e-89f0-959a3394884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextChunker:\n",
    "    \"\"\"\n",
    "    A class to handle text chunking with overlapping windows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int = 1000, overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize the TextChunker with chunking parameters.\n",
    "\n",
    "        Args:\n",
    "            chunk_size (int): Size of each text chunk in characters\n",
    "            overlap (int): Number of overlapping characters between chunks\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into overlapping chunks.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be chunked\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of text chunks\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        end = self.chunk_size\n",
    "\n",
    "        while start < len(text):\n",
    "            chunks.append(text[start:end])\n",
    "            start += self.chunk_size - self.overlap\n",
    "            end = start + self.chunk_size\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4e508-76e1-4793-9712-147cca58338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    A simple in-memory vector store for text embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the VectorStore with empty storage.\n",
    "        \"\"\"\n",
    "        self.embeddings = []\n",
    "        self.texts = []\n",
    "\n",
    "    def add_embeddings(self, embeddings: List[List[float]], texts: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Add embeddings and their corresponding texts to the store.\n",
    "\n",
    "        Args:\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "            texts (List[str]): List of corresponding text chunks\n",
    "        \"\"\"\n",
    "        self.embeddings.extend(embeddings)\n",
    "        self.texts.extend(texts)\n",
    "\n",
    "    def semantic_search(\n",
    "        self, query_embedding: List[float], top_k: int = 3\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Perform semantic search using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): The embedding of the query\n",
    "            top_k (int): Number of top results to return\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[str, float]]: List of tuples containing text and similarity score\n",
    "        \"\"\"\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "\n",
    "        # Convert to numpy arrays for efficient computation\n",
    "        query_array = np.array(query_embedding).reshape(1, -1)\n",
    "        embeddings_array = np.array(self.embeddings)\n",
    "\n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity(query_array, embeddings_array)[0]\n",
    "\n",
    "        # Get top_k results\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "        return [(self.texts[i], similarities[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a5ebd-0fe9-4804-b9fe-2470b0122d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    A simple RAG (Retrieval-Augmented Generation) system implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"llama3.2:3b\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with Ollama components.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the Ollama model to use\n",
    "        \"\"\"\n",
    "        self.embedding_model = OllamaEmbeddings(model=model_name)\n",
    "        self.llm = ChatOllama(model=model_name)\n",
    "        self.vector_store = VectorStore()\n",
    "\n",
    "    def load_and_process_document(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load a PDF document, extract text, chunk it, and generate embeddings.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the PDF file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract text from PDF\n",
    "            extractor = PDFTextExtractor(file_path)\n",
    "            text = extractor.extract_text()\n",
    "\n",
    "            # Chunk the text\n",
    "            chunker = TextChunker()\n",
    "            chunks = chunker.chunk_text(text)\n",
    "\n",
    "            # Generate embeddings for each chunk\n",
    "            embeddings = self.embedding_model.embed_documents(chunks)\n",
    "\n",
    "            # Store embeddings and texts\n",
    "            self.vector_store.add_embeddings(embeddings, chunks)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def retrieve_relevant_chunks(\n",
    "        self, query: str, top_k: int = 3\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text chunks for a given query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query\n",
    "            top_k (int): Number of top results to return\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[str, float]]: List of relevant chunks with similarity scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate embedding for the query\n",
    "            query_embedding = self.embedding_model.embed_query(query)\n",
    "\n",
    "            # Perform semantic search\n",
    "            return self.vector_store.semantic_search(query_embedding, top_k)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving chunks: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def generate_response(self, query: str, context: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using the LLM with the provided context.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query\n",
    "            context (List[str]): List of relevant context chunks\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Combine context chunks\n",
    "            context_str = \"\\n\\n\".join(context)\n",
    "\n",
    "            # Create the prompt\n",
    "            prompt = f\"\"\"Use the following context to answer the question at the end.\n",
    "            If you don't know the answer, just say you don't know, don't try to make up an answer.\n",
    "            \n",
    "            Context:\n",
    "            {context_str}\n",
    "            \n",
    "            Question: {query}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "\n",
    "            # Get response from LLM\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {str(e)}\")\n",
    "            return \"I encountered an error while generating a response.\"\n",
    "\n",
    "    def query(self, question: str, top_k: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Complete RAG pipeline: retrieve relevant chunks and generate response.\n",
    "\n",
    "        Args:\n",
    "            question (str): The user question\n",
    "            top_k (int): Number of chunks to retrieve\n",
    "\n",
    "        Returns:\n",
    "            str: The generated answer\n",
    "        \"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.retrieve_relevant_chunks(question, top_k)\n",
    "\n",
    "        if not relevant_chunks:\n",
    "            return \"I couldn't find any relevant information to answer your question.\"\n",
    "\n",
    "        # Extract just the text (without scores) for generation\n",
    "        context_texts = [chunk[0] for chunk in relevant_chunks]\n",
    "\n",
    "        # Generate response\n",
    "        return self.generate_response(question, context_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755cda03-7a0a-4ba7-a0f7-157d305c073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAGSystem(model_name=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272e80c-255c-4458-a26f-f5d5e217cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./dataset/health supplements/1. dietary supplements - for whom.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e898b00-a800-43c9-9d76-9c12c531f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.load_and_process_document(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243a11a-affb-4592-a355-0ad21478d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic of this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a4264-a7ae-49f3-8198-f9462f46d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag.query(query)\n",
    "print(f\"Response to '{query}':\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbee566-eae9-41e9-abca-d051c34c2e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
