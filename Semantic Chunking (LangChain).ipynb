{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55007ea-7085-40be-a9c1-47c940bc2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f470d-1d76-4f10-8359-b5d077cfe7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticChunking:\n",
    "    \"\"\"\n",
    "    A class to implement Semantic Chunking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"llama3.2:3b\"):\n",
    "        \"\"\"\n",
    "        Initialize the SemanticChunking.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the Ollama model to use\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.embeddings = None\n",
    "        self.vectorstore = None\n",
    "        self.llm = None\n",
    "\n",
    "    def setup_environment(self) -> None:\n",
    "        \"\"\"\n",
    "        Set up the environment and initialize necessary components.\n",
    "        \"\"\"\n",
    "        # Initialize the embedding model\n",
    "        self.embeddings = OllamaEmbeddings(model=self.model_name)\n",
    "\n",
    "        # Initialize the LLM for chat\n",
    "        self.llm = ChatOllama(model=self.model_name, temperature=0.7)\n",
    "\n",
    "        print(\"Environment setup complete with model:\", self.model_name)\n",
    "\n",
    "    def extract_text_from_pdf(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Extract text from a PDF file.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the PDF file\n",
    "\n",
    "        Returns:\n",
    "            List of Document objects containing the extracted text\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the PDF file doesn't exist\n",
    "            Exception: For other extraction errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"PDF file not found at {file_path}\")\n",
    "\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            print(f\"Successfully extracted text from {file_path}\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {e}\")\n",
    "            raise\n",
    "\n",
    "    def calculate_similarity_differences(\n",
    "        self, embeddings: List[List[float]]\n",
    "    ) -> List[float]:\n",
    "        \"\"\"\n",
    "        Calculate similarity differences between consecutive sentence embeddings.\n",
    "\n",
    "        Args:\n",
    "            embeddings: List of sentence embeddings\n",
    "\n",
    "        Returns:\n",
    "            List of similarity difference scores\n",
    "        \"\"\"\n",
    "        similarities = []\n",
    "        for i in range(1, len(embeddings)):\n",
    "            # Calculate cosine similarity between consecutive embeddings\n",
    "            cos_sim = np.dot(embeddings[i - 1], embeddings[i]) / (\n",
    "                np.linalg.norm(embeddings[i - 1]) * np.linalg.norm(embeddings[i])\n",
    "            )\n",
    "            similarities.append(1 - cos_sim)  # Convert to difference\n",
    "\n",
    "        return similarities\n",
    "\n",
    "    def create_semantic_chunks(\n",
    "        self, documents: List[Document], breakpoint_threshold: float = 0.2\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Create semantic chunks from documents using similarity differences.\n",
    "\n",
    "        Args:\n",
    "            documents: List of Document objects to chunk\n",
    "            breakpoint_threshold: Threshold for determining chunk boundaries\n",
    "\n",
    "        Returns:\n",
    "            List of semantically chunked Document objects\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First split into sentences using a standard text splitter\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=100,  # Small chunks for sentences\n",
    "                chunk_overlap=0,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \"],\n",
    "            )\n",
    "            sentences = text_splitter.split_documents(documents)\n",
    "\n",
    "            # Get embeddings for each sentence\n",
    "            sentence_texts = [s.page_content for s in sentences]\n",
    "            sentence_embeddings = self.embeddings.embed_documents(sentence_texts)\n",
    "\n",
    "            # Calculate similarity differences\n",
    "            similarity_diffs = self.calculate_similarity_differences(\n",
    "                sentence_embeddings\n",
    "            )\n",
    "\n",
    "            # Create semantic chunks based on similarity differences\n",
    "            semantic_chunker = SemanticChunker(\n",
    "                embeddings=self.embeddings,\n",
    "                breakpoint_threshold_amount=breakpoint_threshold,\n",
    "            )\n",
    "\n",
    "            # Combine back into full text for semantic chunking\n",
    "            full_text = \"\\n\".join([d.page_content for d in documents])\n",
    "            semantic_chunks = semantic_chunker.create_documents([full_text])\n",
    "\n",
    "            print(f\"Created {len(semantic_chunks)} semantic chunks\")\n",
    "            return semantic_chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic chunking: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings_for_chunks(self, chunks: List[Document]) -> FAISS:\n",
    "        \"\"\"\n",
    "        Create embeddings for semantic chunks and store in a vector database.\n",
    "\n",
    "        Args:\n",
    "            chunks: List of Document objects to embed\n",
    "\n",
    "        Returns:\n",
    "            FAISS vector store containing the chunk embeddings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
    "            print(\"Created embeddings for semantic chunks\")\n",
    "            return self.vectorstore\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "    def perform_semantic_search(self, query: str, k: int = 3) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform semantic search on the stored chunks.\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of chunks to retrieve\n",
    "\n",
    "        Returns:\n",
    "            List of relevant Document objects\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If vectorstore is not initialized\n",
    "        \"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\n",
    "                \"Vector store not initialized. Call create_embeddings_for_chunks first.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            relevant_chunks = self.vectorstore.similarity_search(query, k=k)\n",
    "            print(f\"Retrieved {len(relevant_chunks)} relevant chunks\")\n",
    "            return relevant_chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic search: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_response(self, query: str, relevant_chunks: List[Document]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response based on retrieved chunks.\n",
    "\n",
    "        Args:\n",
    "            query: User query\n",
    "            relevant_chunks: List of relevant Document objects\n",
    "\n",
    "        Returns:\n",
    "            Generated response as a string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Combine the chunks into context\n",
    "            context = \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "\n",
    "            # Create a prompt template\n",
    "            prompt = ChatPromptTemplate.from_template(\n",
    "                \"\"\"Answer the following question based only on the provided context:\n",
    "                \n",
    "                Context:\n",
    "                {context}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Answer in a clear and concise manner. If you don't know the answer, \n",
    "                simply say you don't know.\"\"\"\n",
    "            )\n",
    "\n",
    "            # Create the chain\n",
    "            chain = prompt | self.llm | StrOutputParser()\n",
    "\n",
    "            # Generate the response\n",
    "            response = chain.invoke({\"question\": query, \"context\": context})\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86cf81-90e2-437b-89cf-017b47a02801",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./dataset/health supplements/1. dietary supplements - for whom.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39cf5c-a5f5-480b-bedf-305d86736e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = SemanticChunking(model_name=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69176ed-eeec-4a2e-b051-f783af8ab4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0e978-392c-4e0b-8194-23dffdde0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = rag.extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8253d6e-3935-4a46-8382-0fbad7db0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chunks = rag.create_semantic_chunks(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550bb1e-c5b5-4cce-a004-885fe6e5bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.create_embeddings_for_chunks(semantic_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe3406-9494-4ba4-b9bd-f9331469522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic of this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f7aa9-d50a-44cc-8ea2-839c5385e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_chunks = rag.perform_semantic_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215b479-f316-498f-be1a-7badaba7838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag.generate_response(query, relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e06e5d-a3fc-4d6c-893c-14a471697c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Response to '{query}':\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364841c-1804-472a-8da7-cc1dd8c844cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
