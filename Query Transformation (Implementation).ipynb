{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f3fbb-8684-48e7-8157-d0f9883c437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import fitz\n",
    "import numpy as np\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1ba09-936d-424c-b625-754bf837c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"A class to represent a document with text content and metadata.\"\"\"\n",
    "\n",
    "    page_content: str\n",
    "    metadata: Dict[str, str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b4544-bb7f-4660-aac5-8b2f7ef9ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFReader:\n",
    "    \"\"\"\n",
    "    A class to read and process PDF documents using PyMuPDF (fitz).\n",
    "\n",
    "    Attributes:\n",
    "        chunk_size (int): Number of characters per text chunk\n",
    "        chunk_overlap (int): Number of overlapping characters between chunks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize the PDF reader with chunking parameters.\n",
    "\n",
    "        Args:\n",
    "            chunk_size (int): Size of each text chunk in characters (default: 1000)\n",
    "            chunk_overlap (int): Overlap between chunks in characters (default: 200)\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def read_pdf(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Read a PDF file and return chunks of text as Documents.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the PDF file\n",
    "\n",
    "        Returns:\n",
    "            List of Document objects containing text chunks and metadata\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the PDF file doesn't exist\n",
    "            Exception: For other PDF reading errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            documents = []\n",
    "            doc = fitz.open(file_path)\n",
    "\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                text = page.get_text()\n",
    "\n",
    "                # Split text into chunks with overlap\n",
    "                chunks = self._chunk_text(text)\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    metadata = {\n",
    "                        \"source\": file_path,\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"chunk_size\": len(chunk),\n",
    "                    }\n",
    "                    documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "            print(f\"Read {len(documents)} chunks from {file_path}\")\n",
    "            return documents\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"PDF file not found: {file_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF file {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into chunks of specified size with overlap.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to chunk\n",
    "\n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        text_length = len(text)\n",
    "\n",
    "        while start < text_length:\n",
    "            end = min(start + self.chunk_size, text_length)\n",
    "            chunks.append(text[start:end])\n",
    "\n",
    "            if end == text_length:\n",
    "                break\n",
    "\n",
    "            start = end - self.chunk_overlap\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249b2f6-60bc-4b30-952c-31feef37bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    A simple in-memory vector store implementation for demonstration purposes.\n",
    "\n",
    "    Attributes:\n",
    "        documents (List[Document]): List of stored documents\n",
    "        embeddings (List[np.ndarray]): Corresponding embeddings for documents\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize an empty vector store.\"\"\"\n",
    "        self.documents: List[Document] = []\n",
    "        self.embeddings: List[np.ndarray] = []\n",
    "\n",
    "    def add_documents(\n",
    "        self, documents: List[Document], embeddings: List[np.ndarray]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): List of Document objects to add\n",
    "            embeddings (List[np.ndarray]): List of corresponding embeddings as numpy arrays\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If lengths of documents and embeddings don't match\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        self.documents.extend(documents)\n",
    "        self.embeddings.extend(embeddings)\n",
    "        print(f\"Added {len(documents)} documents to the vector store\")\n",
    "\n",
    "    def similarity_search(\n",
    "        self, query_embedding: np.ndarray, k: int = 3\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Perform similarity search using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (np.ndarray): Embedding of the query as numpy array\n",
    "            k (int): Number of top results to return (default: 3)\n",
    "\n",
    "        Returns:\n",
    "            List of tuples containing (Document, similarity_score) ordered by similarity\n",
    "        \"\"\"\n",
    "        if not self.documents:\n",
    "            raise ValueError(\"Vector store is empty\")\n",
    "\n",
    "        # Calculate cosine similarities\n",
    "        similarities = []\n",
    "        for doc_embedding in self.embeddings:\n",
    "            cosine_sim = np.dot(query_embedding, doc_embedding) / (\n",
    "                norm(query_embedding) * norm(doc_embedding)\n",
    "            )\n",
    "            similarities.append(cosine_sim)\n",
    "\n",
    "        # Get top k results\n",
    "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        results = [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "        print(f\"Retrieved {len(results)} documents from vector store\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c82de-ca26-4f02-a19d-f2c32f155a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryTransformer:\n",
    "    \"\"\"Base class for query transformation techniques.\"\"\"\n",
    "\n",
    "    def __init__(self, llm_client: ChatOllama):\n",
    "        \"\"\"\n",
    "        Initialize the query transformer with an LLM client.\n",
    "\n",
    "        Args:\n",
    "            llm_client (ChatOllama): Initialized ChatOllama client\n",
    "        \"\"\"\n",
    "        self.llm = llm_client\n",
    "\n",
    "    def transform(self, query: str) -> str:\n",
    "        \"\"\"Transform the input query (to be implemented by subclasses).\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41064025-3b94-44cd-8b51-f133d76de375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRewriter(QueryTransformer):\n",
    "    \"\"\"Implements query rewriting technique to make queries more specific.\"\"\"\n",
    "\n",
    "    def transform(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Rewrite the query to be more specific and detailed.\n",
    "\n",
    "        Args:\n",
    "            query (str): Original user query\n",
    "\n",
    "        Returns:\n",
    "            Rewritten, more specific query\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful query rewriter. Your task is to make the following query more specific \n",
    "        and detailed while preserving the original intent. Add relevant details that would help \n",
    "        retrieve more precise information.\n",
    "        \n",
    "        Original query: {query}\n",
    "        \n",
    "        Rewritten query:\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            rewritten_query = response.content.strip()\n",
    "            print(f\"Rewritten query: {rewritten_query}\")\n",
    "            return rewritten_query\n",
    "        except Exception as e:\n",
    "            print(f\"Error in query rewriting: {e}\")\n",
    "            return query  # Fallback to original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eabbfb-17e5-4c5d-9e50-a8a25951af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepBackPrompting(QueryTransformer):\n",
    "    \"\"\"Implements step-back prompting to generate broader contextual queries.\"\"\"\n",
    "\n",
    "    def transform(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a broader query to retrieve contextual background information.\n",
    "\n",
    "        Args:\n",
    "            query (str): Original user query\n",
    "\n",
    "        Returns:\n",
    "            Broader, more conceptual query for context retrieval\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful assistant that generates step-back questions. For the given query, \n",
    "        create a broader question that asks about the higher-level concepts or principles \n",
    "        needed to answer the original query.\n",
    "        \n",
    "        Original query: {query}\n",
    "        \n",
    "        Step-back question:\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            step_back_query = response.content.strip()\n",
    "            print(f\"Step-back query: {step_back_query}\")\n",
    "            return step_back_query\n",
    "        except Exception as e:\n",
    "            print(f\"Error in step-back prompting: {e}\")\n",
    "            return query  # Fallback to original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89c652-d19d-42b1-ba3e-e5dd7ad17b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubQueryDecomposer(QueryTransformer):\n",
    "    \"\"\"Implements sub-query decomposition to break complex queries into simpler parts.\"\"\"\n",
    "\n",
    "    def transform(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Break down a complex query into simpler sub-queries.\n",
    "\n",
    "        Args:\n",
    "            query (str): Original complex user query\n",
    "\n",
    "        Returns:\n",
    "            List of simpler sub-queries that cover aspects of the original query\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful query decomposer. Break down the following complex query into \n",
    "        2-3 simpler sub-queries that cover different aspects of the original query.\n",
    "        Return each sub-query on a new line.\n",
    "        \n",
    "        Original query: {query}\n",
    "        \n",
    "        Generate 3 sub-queries, one per line, in this format:\n",
    "        1. [First sub-query]\n",
    "        2. [Second sub-query]\n",
    "        3. [Third sub-query]\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            sub_queries = [q.strip() for q in response.content.split(\"\\n\") if q.strip()]\n",
    "            print(f\"Generated sub-queries: {sub_queries}\")\n",
    "            return sub_queries\n",
    "        except Exception as e:\n",
    "            print(f\"Error in query decomposition: {e}\")\n",
    "            return [query]  # Fallback to original query as single sub-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbf0c6-c39d-4b75-a5d2-cf641f7b9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    A RAG (Retrieval-Augmented Generation) system with query transformation capabilities.\n",
    "\n",
    "    Attributes:\n",
    "        llm: ChatOllama instance for generation\n",
    "        embeddings: OllamaEmbeddings instance for creating embeddings\n",
    "        vector_store: VectorStore for document retrieval\n",
    "        query_transformers: Dictionary of available query transformation techniques\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm_model: str = \"llama3.2:3b\", embedding_model: str = \"llama3\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with LLM and embedding models.\n",
    "\n",
    "        Args:\n",
    "            llm_model: Name of the Ollama LLM model to use\n",
    "            embedding_model: Name of the Ollama embedding model to use\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.llm = ChatOllama(model=llm_model, temperature=0.3)\n",
    "            self.embeddings = OllamaEmbeddings(model=embedding_model)\n",
    "            self.vector_store = VectorStore()\n",
    "            self.pdf_reader = PDFReader()\n",
    "\n",
    "            # Initialize query transformers\n",
    "            self.query_transformers = {\n",
    "                \"rewriting\": QueryRewriter(self.llm),\n",
    "                \"step_back\": StepBackPrompting(self.llm),\n",
    "                \"decomposition\": SubQueryDecomposer(self.llm),\n",
    "            }\n",
    "\n",
    "            print(\"RAG system initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize RAG system: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_pdf_documents(self, file_paths: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Add PDF documents to the vector store after reading and chunking.\n",
    "\n",
    "        Args:\n",
    "            file_paths (List[str]): List of paths to PDF files\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no valid PDF files are provided\n",
    "        \"\"\"\n",
    "        if not file_paths:\n",
    "            raise ValueError(\"No PDF files provided\")\n",
    "\n",
    "        all_documents = []\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                documents = self.pdf_reader.read_pdf(file_path)\n",
    "                all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {file_path} due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not all_documents:\n",
    "            raise ValueError(\"No valid PDF documents could be processed\")\n",
    "\n",
    "        self.add_documents(all_documents)\n",
    "\n",
    "    def add_documents(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Add documents to the vector store after generating their embeddings.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): List of Document objects to add\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate embeddings for each document\n",
    "            texts = [doc.page_content for doc in documents]\n",
    "            doc_embeddings = self.embeddings.embed_documents(texts)\n",
    "            doc_embeddings = [np.array(embedding) for embedding in doc_embeddings]\n",
    "\n",
    "            # Add to vector store\n",
    "            self.vector_store.add_documents(documents, doc_embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def retrieve(\n",
    "        self, query: str, transformation: Optional[str] = None, k: int = 3\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents with optional query transformation.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to search with\n",
    "            transformation (str): Optional query transformation technique to apply\n",
    "                           ('rewriting', 'step_back', 'decomposition', or None)\n",
    "            k (int): Number of documents to retrieve\n",
    "\n",
    "        Returns:\n",
    "            List of (Document, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Apply query transformation if specified\n",
    "            transformed_queries = [query]\n",
    "\n",
    "            if transformation and transformation in self.query_transformers:\n",
    "                transformer = self.query_transformers[transformation]\n",
    "\n",
    "                if transformation == \"decomposition\":\n",
    "                    transformed_queries = transformer.transform(query)\n",
    "                else:\n",
    "                    transformed_query = transformer.transform(query)\n",
    "                    transformed_queries = [transformed_query]\n",
    "\n",
    "            # Retrieve documents for each transformed query\n",
    "            all_results = []\n",
    "            for q in transformed_queries:\n",
    "                # Generate embedding for the query\n",
    "                query_embedding = np.array(self.embeddings.embed_query(q))\n",
    "\n",
    "                # Perform similarity search\n",
    "                results = self.vector_store.similarity_search(query_embedding, k=k)\n",
    "                all_results.extend(results)\n",
    "\n",
    "            # Deduplicate and sort results by similarity score\n",
    "            unique_results = list(\n",
    "                {doc.page_content: (doc, score) for doc, score in all_results}.values()\n",
    "            )\n",
    "            unique_results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            return unique_results[:k]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "    def generate_response(self, query: str, retrieved_docs: List[Document]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using the LLM based on the query and retrieved documents.\n",
    "\n",
    "        Args:\n",
    "            query (str): Original user query\n",
    "            retrieved_docs (List[Document]): List of relevant documents retrieved\n",
    "\n",
    "        Returns:\n",
    "            Generated response as a string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Format the retrieved documents as context\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            You are a helpful assistant that answers questions based on the provided context.\n",
    "            Use the following context to answer the question at the end. If you don't know\n",
    "            the answer, just say you don't know - don't make up an answer.\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {query}\n",
    "            \n",
    "            Answer:\n",
    "            \"\"\"\n",
    "\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return response.content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in response generation: {e}\")\n",
    "            return (\n",
    "                \"I encountered an error while generating a response. Please try again.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a9e7f-8961-4c9d-93d5-3a2f966fa31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAGSystem(llm_model=\"llama3.2:3b\", embedding_model=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1ae4b-3ff2-42e5-bbe6-c11b0a8116be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = [\"./dataset/health supplements/1. dietary supplements - for whom.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ed0bb-cbb1-4953-a2b0-e0306f0867d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.add_pdf_documents(pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b92da-a3bf-43c4-ac22-bc73738bba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic discussed in section 3?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540dfbd5-8b13-4ea3-ab35-86856b74a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Original Query ===\")\n",
    "print(query + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6d371-04d7-482d-b562-e7aed3e2f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Query Rewriting ===\")\n",
    "rewritten_query = rag.query_transformers[\"rewriting\"].transform(query)\n",
    "print(rewritten_query + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34306e-56c8-4cbb-b76f-8c9171c9e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Step-back Prompting ===\")\n",
    "step_back_query = rag.query_transformers[\"step_back\"].transform(query)\n",
    "print(step_back_query + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42f6a2-7b2d-4c18-9f12-6c28386db5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Sub-query Decomposition ===\")\n",
    "sub_queries = rag.query_transformers[\"decomposition\"].transform(query)\n",
    "for i, q in enumerate(sub_queries, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5802f-632d-4582-8d4f-ba07cc2e2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RAG with Query Rewriting ===\")\n",
    "retrieved_docs = rag.retrieve(query, transformation=\"step_back\")\n",
    "response = rag.generate_response(query, [doc for doc, _ in retrieved_docs])\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71afe40-5527-4a95-b996-39ddddf21726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
