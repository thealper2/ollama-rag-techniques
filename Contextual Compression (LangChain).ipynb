{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496e186-6b00-4733-ae4f-eccbd118af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e0c60-0891-455f-955f-ea6978b4b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    \"\"\"\n",
    "    A class to handle the RAG pipeline with contextual compression.\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): Name of the Ollama model to use\n",
    "        embeddings_model (Embeddings): Embeddings model instance\n",
    "        llm (ChatOllama): LLM instance for generation\n",
    "        text_splitter (RecursiveCharacterTextSplitter): Text splitter for chunking\n",
    "        vectorstore (Optional[FAISS]): Vector store for document embeddings\n",
    "        retriever (Optional[BaseRetriever]): Document retriever\n",
    "        compression_retriever (Optional[ContextualCompressionRetriever]): Compressed retriever\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"llama3.2:3b\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the Ollama model to use (default: \"llama3.2:3b\")\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.embeddings_model: Optional[Embeddings] = None\n",
    "        self.llm: Optional[ChatOllama] = None\n",
    "        self.text_splitter: Optional[RecursiveCharacterTextSplitter] = None\n",
    "        self.vectorstore: Optional[FAISS] = None\n",
    "        self.retriever: Optional[BaseRetriever] = None\n",
    "        self.compression_retriever: Optional[ContextualCompressionRetriever] = None\n",
    "        self._setup_environment()\n",
    "\n",
    "    def _setup_environment(self) -> None:\n",
    "        \"\"\"Set up the required models and components.\"\"\"\n",
    "        try:\n",
    "            # Initialize Ollama embeddings\n",
    "            self.embeddings_model = OllamaEmbeddings(model=self.model_name)\n",
    "\n",
    "            # Initialize Ollama LLM\n",
    "            self.llm = ChatOllama(model=self.model_name)\n",
    "\n",
    "            # Initialize text splitter\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200,\n",
    "                length_function=len,\n",
    "                is_separator_regex=False,\n",
    "            )\n",
    "\n",
    "            print(\"Environment setup completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up environment: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_text_from_pdf(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Extract text from a PDF file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the PDF file\n",
    "\n",
    "        Returns:\n",
    "            List of extracted documents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not Path(file_path).exists():\n",
    "                raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            print(f\"Extracted {len(documents)} pages from PDF.\")\n",
    "            return documents\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {e}\")\n",
    "            raise\n",
    "\n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split document into chunks.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): List of documents to chunk\n",
    "\n",
    "        Returns:\n",
    "            List of chunked documents\n",
    "        \"\"\"\n",
    "        if not self.text_splitter:\n",
    "            raise ValueError(\"Text spliiter not initialized.\")\n",
    "\n",
    "        try:\n",
    "            chunks = self.text_splitter.split_documents(documents)\n",
    "            print(f\"Split documents into {len(chunks)} chunks.\")\n",
    "            return chunks\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error chunking documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, chunks: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Create embeddings for document chunks and store them in a vector store.\n",
    "\n",
    "        Args:\n",
    "            chunks (List[Document]): List of document chunks to embed\n",
    "        \"\"\"\n",
    "        if not self.embeddings_model:\n",
    "            raise ValueError(\"Embeddings model not initialized.\")\n",
    "\n",
    "        try:\n",
    "            self.vectorstore = FAISS.from_documents(chunks, self.embeddings_model)\n",
    "            self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "            print(\"Created embeddings and initialized retriever.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "    def setup_contextual_compression(self) -> None:\n",
    "        \"\"\"\n",
    "        Set up contextual compression for the retriever.\n",
    "        \"\"\"\n",
    "        if not self.llm or not self.retriever:\n",
    "            raise ValueError(\"LLM or retriever not initialized.\")\n",
    "\n",
    "        try:\n",
    "            # Create a document compressor\n",
    "            compressor = LLMChainExtractor.from_llm(self.llm)\n",
    "\n",
    "            # Create the compression retriever\n",
    "            self.compression_retriever = ContextualCompressionRetriever(\n",
    "                base_compressor=compressor, base_retriever=self.retriever\n",
    "            )\n",
    "            print(\"Contextual compression retriever setup completed.\")\n",
    "\n",
    "        except Excetion as e:\n",
    "            print(f\"Error setting up contextual compression: {e}\")\n",
    "            raise\n",
    "\n",
    "    def retrieve_documents(self, query: str, compressed: bool = True) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to search for\n",
    "            compressed (bool): Whether to use compressed retrieval (default: True)\n",
    "\n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if compressed:\n",
    "                if not self.compression_retriever:\n",
    "                    raise ValueError(\"Compression retriever not initialized.\")\n",
    "                return self.compression_retriever.invoke(query)\n",
    "            else:\n",
    "                if not self.retriever:\n",
    "                    raise ValueError(\"Retriever not initialized.\")\n",
    "                return self.retriever.invoke(query)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_response(self, query: str, compressed: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response to a query using RAG.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to respond to\n",
    "            compressed (bool): Whether to use compressed retrieval (default: True)\n",
    "\n",
    "        Returns:\n",
    "            The generated response\n",
    "        \"\"\"\n",
    "        if not self.llm:\n",
    "            raise ValueError(\"LLM not initialized.\")\n",
    "\n",
    "        try:\n",
    "            # Retrieve relevant documents\n",
    "            retrieved_docs = self.retrieve_documents(query, compressed)\n",
    "\n",
    "            # Format the documents as context\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "            # Create a prompt template\n",
    "            prompt = ChatPromptTemplate.from_template(\n",
    "                \"\"\"Answer the following question based on the provided context.\n",
    "                Be concise and accurate. If you don't know the answer, say you don't know.\n",
    "                \n",
    "                Context: {context}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "\n",
    "            # Create the RAG chain\n",
    "            rag_chain = (\n",
    "                {\"context\": lambda x: context, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | self.llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            # Generate the response\n",
    "            response = rag_chain.invoke(query)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e18b2-3da7-4bb8-849b-aaf63d52c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./dataset/health supplements/1. dietary supplements - for whom.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabefaa-0b51-42e2-8ce1-b358ec29be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(model_name=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85121d-796d-419d-9b50-10e8dc447b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = rag.extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed74a6e-9174-4026-ad68-48064f0511b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = rag.chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b347be4-a3be-44ae-80b0-5696734c834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a500983-76b7-4eec-b597-9d9528ae3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.setup_contextual_compression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3966c7d-29fd-4ba3-b16b-851a4c0c5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic discussed in section 3?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728eccdd-b6bd-4df8-ada2-1e489af975de",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_docs = rag.retrieve_documents(query, compressed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68956bc3-9887-43ef-a60c-cd633ef6635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs = rag.retrieve_documents(query, compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede6fc6-2134-4d3d-b067-123cdff0af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Regular retrieval returned {len(regular_docs)} documents\")\n",
    "print(f\"Compressed retrieval returned {len(compressed_docs)} documents\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110aa4f-890a-4d9e-b449-c3a63753c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag.generate_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92ec3b-8dbc-4d10-8b0c-fc8fa237affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Response to query '{query}':\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840aef8-a4c9-4e1f-989d-0a059c4d7d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e536746-edc1-4526-a3b3-47554a8c7c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
