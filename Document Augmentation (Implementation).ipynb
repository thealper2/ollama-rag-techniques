{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0cebad-f144-43fe-9b20-c364adfe7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import fitz\n",
    "import numpy as np\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291b3d6-66f5-42e6-bfe7-c1b1694559a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    \"\"\"\n",
    "    A class to implement Document Augmentation in RAG (Retrieval-Augmented Generation) pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"llama3.2:3b\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG pipeline with Ollama models.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the Ollama model to use (default: \"llama3.2:3b\")\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model = OllamaEmbeddings(model=model_name)\n",
    "        self.llm = ChatOllama(model=model_name)\n",
    "        self.chunks: List[str] = []\n",
    "        self.augmented_chunks: List[str] = []\n",
    "        self.embeddings: List[List[float]] = []\n",
    "        self.augmented_embeddings: List[List[float]] = []\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract text content from a PDF file.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF file\n",
    "\n",
    "        Returns:\n",
    "            Extracted text as a single string\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If PDF file doesn't exist\n",
    "            ValueError: If PDF extraction fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(pdf_path):\n",
    "                raise FileNotFoundError(f\"PDF file not found at {pdf_path}\")\n",
    "\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = \"\\n\".join(page.get_text() for page in doc)\n",
    "\n",
    "            if not text.strip():\n",
    "                raise ValueError(\"No text could be extracted from the PDF\")\n",
    "\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error extracting text from PDF: {str(e)}\")\n",
    "\n",
    "    def chunk_text(\n",
    "        self, text: str, chunk_size: int = 1000, overlap: int = 200\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into chunks with optional overlap.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to chunk\n",
    "            chunk_size (int): Maximum size of each chunk in characters (default: 1000)\n",
    "            overlap (int): Number of overlapping characters between chunks (default: 200)\n",
    "\n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        # Clean the text by removing excessive whitespace\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        chunks = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(text):\n",
    "            end = min(start + chunk_size, len(text))\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "\n",
    "            # Stop if we've reached the end of the text\n",
    "            if end == len(text):\n",
    "                break\n",
    "\n",
    "            # Move the start position, accounting for overlap\n",
    "            start = end - overlap\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def augment_chunk(self, chunk: str) -> str:\n",
    "        \"\"\"\n",
    "        Augment a text chunk by generating questions and answers about its content.\n",
    "\n",
    "        Args:\n",
    "            chunk (str): Text chunk to augment\n",
    "\n",
    "        Returns:\n",
    "            Augmented chunk with Q&A section\n",
    "        \"\"\"\n",
    "        # Prompt to generate questions about the chunk\n",
    "        prompt = f\"\"\"Based on the following text, generate 2-3 relevant questions and their answers.\n",
    "        Format the output exactly as:\n",
    "        [Original Text]\n",
    "        {{original text}}\n",
    "        \n",
    "        [Questions and Answers]\n",
    "        1. Q: {{question}}\n",
    "           A: {{answer}}\n",
    "        2. Q: {{question}}\n",
    "           A: {{answer}}\n",
    "        \n",
    "        Text: {chunk}\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            augmented_text = response.content\n",
    "\n",
    "            # Combine original and augmented content\n",
    "            return f\"{chunk}\\n\\n---AUGMENTED---\\n{augmented_text}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error augmenting chunk: {str(e)}\")\n",
    "            return chunk  # Return original if augmentation fails\n",
    "\n",
    "    def generate_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Generate embedding for a given text using Ollama.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to embed\n",
    "\n",
    "        Returns:\n",
    "            Embedding vector as list of floats\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.embedding_model.embed_query(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def cosine_similarity(self, vec_a: List[float], vec_b: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two vectors.\n",
    "\n",
    "        Args:\n",
    "            vec_a (List[float]): First vector\n",
    "            vec_b (List[float]): Second vector\n",
    "\n",
    "        Returns:\n",
    "            Cosine similarity score between -1 and 1\n",
    "        \"\"\"\n",
    "        if not vec_a or not vec_b or len(vec_a) != len(vec_b):\n",
    "            return 0.0\n",
    "\n",
    "        return np.dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))\n",
    "\n",
    "    def process_document(self, pdf_path: str):\n",
    "        \"\"\"\n",
    "        Process a PDF document: extract, chunk, augment, and generate embeddings.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF file\n",
    "        \"\"\"\n",
    "        print(\"Processing document...\")\n",
    "\n",
    "        # Step 1: Extract text from PDF\n",
    "        text = self.extract_text_from_pdf(pdf_path)\n",
    "        print(f\"Extracted {len(text)} characters from PDF\")\n",
    "\n",
    "        # Step 2: Chunk the text\n",
    "        self.chunks = self.chunk_text(text)\n",
    "        print(f\"Created {len(self.chunks)} text chunks\")\n",
    "\n",
    "        # Step 3: Augment chunks\n",
    "        self.augmented_chunks = [self.augment_chunk(chunk) for chunk in self.chunks]\n",
    "        print(f\"Augmented {len(self.augmented_chunks)} chunks\")\n",
    "\n",
    "        # Step 4: Generate embeddings for original chunks\n",
    "        self.embeddings = [self.generate_embedding(chunk) for chunk in self.chunks]\n",
    "\n",
    "        # Step 5: Generate embeddings for augmented chunks\n",
    "        self.augmented_embeddings = [\n",
    "            self.generate_embedding(chunk) for chunk in self.augmented_chunks\n",
    "        ]\n",
    "\n",
    "        print(f\"Generated {len(self.embeddings)} original embeddings\")\n",
    "        print(f\"Generated {len(self.augmented_embeddings)} augmented embeddings\")\n",
    "\n",
    "    def retrieve_relevant_chunks(\n",
    "        self, query: str, use_augmented: bool = False, top_k: int = 3\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks based on query similarity.\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query\n",
    "            use_augmented (bool): Whether to use augmented chunks (default: False)\n",
    "            top_k (int): Number of top chunks to return\n",
    "\n",
    "        Returns:\n",
    "            List of tuples containing (chunk_text, similarity_score)\n",
    "        \"\"\"\n",
    "        # Generate embedding for the query\n",
    "        query_embedding = self.generate_embedding(query)\n",
    "        if not query_embedding:\n",
    "            return []\n",
    "\n",
    "        # Choose which embeddings and chunks to use\n",
    "        embeddings = self.augmented_embeddings if use_augmented else self.embeddings\n",
    "        chunks = self.augmented_chunks if use_augmented else self.chunks\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        similarities = []\n",
    "        for i, emb in enumerate(embeddings):\n",
    "            if emb:  # Only process if we have a valid embedding\n",
    "                sim = self.cosine_similarity(query_embedding, emb)\n",
    "                similarities.append((chunks[i], sim))\n",
    "\n",
    "        # Sort by similarity (descending) and return top_k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "    def generate_response(self, query: str, use_augmented: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response to a query using RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            query (str): User query\n",
    "            use_augmented (bool): Whether to use augmented chunks (default: False)\n",
    "\n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.retrieve_relevant_chunks(query, use_augmented)\n",
    "\n",
    "        if not relevant_chunks:\n",
    "            return \"I couldn't find any relevant information to answer your question.\"\n",
    "\n",
    "        # Prepare context for the LLM\n",
    "        context = \"\\n\\n\".join([chunk for chunk, _ in relevant_chunks])\n",
    "\n",
    "        # Generate prompt with context\n",
    "        prompt = f\"\"\"Use the following context to answer the question at the end.\n",
    "        If you don't know the answer, just say you don't know, don't try to make up an answer.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30332fa8-30bf-4dd9-ad18-d6c58193d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./dataset/health supplements/1. dietary supplements - for whom.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23c353-aec8-4a70-b051-8b14479164d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(model_name=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeec422-1fb5-4b88-97ca-256887c1849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.process_document(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f18473-dc27-4eee-b6cc-cb88547e7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vector Database Statistics:\")\n",
    "print(f\"- Total original chunks: {len(rag.chunks)}\")\n",
    "print(f\"- Total augmented chunks: {len(rag.augmented_chunks)}\")\n",
    "print(f\"- Total original embeddings: {len(rag.embeddings)}\")\n",
    "print(f\"- Total augmented embeddings: {len(rag.augmented_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a51a0-c50c-48cf-ab6c-5ed192674bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"Provide information about the functionality and safety of dieatary supplements.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3210f-c0cd-40b2-abc2-3e067d0280c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nRunning query: '{query}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a55ab7-5ba5-4806-8d64-0f5ed3ee6ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nResults with ORIGINAL data:\")\n",
    "original_chunks = rag.retrieve_relevant_chunks(query, use_augmented=False)\n",
    "print(\"\\nMost relevant original chunks:\")\n",
    "for i, (chunk, score) in enumerate(original_chunks, 1):\n",
    "    print(f\"\\nChunk {i} (Similarity: {score:.2f}):\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762239a-1971-4faf-814f-216f6c0396ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_response = rag.generate_response(query, use_augmented=False)\n",
    "print(\"\\nGenerated response (original):\")\n",
    "print(original_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b055930-ccdb-4b89-8c02-707f9d07245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nResults with AUGMENTED data:\")\n",
    "augmented_chunks = rag.retrieve_relevant_chunks(query, use_augmented=True)\n",
    "print(\"\\nMost relevant augmented chunks:\")\n",
    "for i, (chunk, score) in enumerate(augmented_chunks, 1):\n",
    "    print(f\"\\nChunk {i} (Similarity: {score:.2f}):\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9423da-e9f8-4283-8ed3-540fac76ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_response = rag.generate_response(query, use_augmented=True)\n",
    "print(\"\\nGenerated response (augmented):\")\n",
    "print(augmented_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6498864-b2d4-4e11-aa2a-58114e7f3850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
