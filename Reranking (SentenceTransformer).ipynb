{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816ab9d-5e19-4544-af08-0b810140ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import fitz\n",
    "import numpy as np\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09aa30-4ebe-45f5-9f1f-358c9ebf0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtractor:\n",
    "    \"\"\"\n",
    "    A class to handle PDF text extraction and preprocessing.\n",
    "\n",
    "    Attributes:\n",
    "        file_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of text chunks in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, file_path: str, chunk_size: int = 1000, chunk_overlap: int = 200\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the PDF text extractor.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the PDF file.\n",
    "            chunk_size (int): Size of text chunks in characters\n",
    "            chunk_overlap (int): Overlap between in characters\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def extract_text(self) -> str:\n",
    "        \"\"\"\n",
    "        Extract raw text from the PDF file.\n",
    "\n",
    "        Returns:\n",
    "            Extracted text as a single string\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the PDF file doesn't exist\n",
    "            Exception: For other PDF reading errors\n",
    "        \"\"\"\n",
    "        if not Path(self.file_path).exists():\n",
    "            raise FileNotFoundError(f\"PDF file not found at {self.file_path}\")\n",
    "\n",
    "        try:\n",
    "            doc = fitz.open(self.file_path)\n",
    "            text = \"\\n\".join(page.get_text() for page in doc)\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error reading PDF: {str(e)}\")\n",
    "\n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into overlapping chunks of specified size.\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to be chunked\n",
    "\n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            chunks.append(text[start:end])\n",
    "            if end == len(text):\n",
    "                break\n",
    "            start = end - self.chunk_overlap\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Complete processing pipeline: extract and chunk text.\n",
    "\n",
    "        Returns:\n",
    "            List of processed text chunks\n",
    "        \"\"\"\n",
    "        raw_text = self.extract_text()\n",
    "        return self.chunk_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b5993-96f5-4a56-b9bc-61ee7b0259ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    A simple in-memory vector store for document chunks and their embeddings.\n",
    "\n",
    "    Attributes:\n",
    "        embeddings_model (OllamaEmbeddings): Model for generating embeddings\n",
    "        chunks (List[str]): List of document chunks\n",
    "        embeddings (List[List[float]]): Corresponding embeddings for each chunk\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings_model: OllamaEmbeddings):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "\n",
    "        Args:\n",
    "            embeddings_model (OllamaEmbeddings): Initialized Ollama embeddings model\n",
    "        \"\"\"\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.chunks = []\n",
    "        self.embeddings = []\n",
    "\n",
    "    def add_documents(self, chunks: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Add documents to the store and generate their embeddings.\n",
    "\n",
    "        Args:\n",
    "            chunks (List[str]): List of text chunks to add\n",
    "        \"\"\"\n",
    "        self.chunks.extend(chunks)\n",
    "        # Generate embeddings for new chunks\n",
    "        new_embeddings = self.embeddings_model.embed_documents(chunks)\n",
    "        self.embeddings.extend(new_embeddings)\n",
    "\n",
    "    def similarity_search(\n",
    "        self, query_embedding: List[float], top_k: int = 5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Find most similar documents to the query using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): Embedding of the query\n",
    "            top_k (int): Number of top results to return\n",
    "\n",
    "        Returns:\n",
    "            List of tuples (chunk, similarity_score)\n",
    "        \"\"\"\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "\n",
    "        # Convert to numpy arrays for efficient computation\n",
    "        query_arr = np.array(query_embedding)\n",
    "        embeddings_arr = np.array(self.embeddings)\n",
    "\n",
    "        # Calculate cosine similarities\n",
    "        norms = np.linalg.norm(embeddings_arr, axis=1) * np.linalg.norm(query_arr)\n",
    "        similarities = np.dot(embeddings_arr, query_arr) / norms\n",
    "\n",
    "        # Get top_k results\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(self.chunks[i], similarities[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6b308-fa7d-46a3-851b-896a8941d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reranker:\n",
    "    \"\"\"\n",
    "    Handles reranking of retrieved documents using a cross-encoder model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the reranker with a cross-encoder model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the sentence-transformers cross-encoder model\n",
    "        \"\"\"\n",
    "        self.model = CrossEncoder(model_name)\n",
    "\n",
    "    def rerank(self, query: str, documents: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Rerank documents based on their relevance to the query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query\n",
    "            documents (List[str]): List of document chunks to rerank\n",
    "\n",
    "        Returns:\n",
    "            List of tuples (document, relevance_score) sorted by relevance\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "\n",
    "        # Create query-document pairs for cross-encoder\n",
    "        pairs = [[query, doc] for doc in documents]\n",
    "\n",
    "        # Get scores from cross-encoder\n",
    "        scores = self.model.predict(pairs)\n",
    "\n",
    "        # Combine documents with their scores and sort\n",
    "        scored_docs = list(zip(documents, scores))\n",
    "        return sorted(scored_docs, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52ec10-950c-4949-9221-cbd77e401062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    \"\"\"\n",
    "    Complete RAG system with retrieval, reranking, and generation components.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdf_path: str,\n",
    "        embeddings_model_name: str = \"llama3.2:3b\",\n",
    "        llm_model_name: str = \"llama3.2:3b\",\n",
    "        cross_encoder_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF document\n",
    "            embeddings_model_name (str): Name of the Ollama embeddings model\n",
    "            llm_model_name (str): Name of the Ollama LLM model\n",
    "            cross_encoder_model (str): Name of the cross-encoder model for reranking\n",
    "        \"\"\"\n",
    "        # Initialize models\n",
    "        self.embeddings_model = OllamaEmbeddings(model=embeddings_model_name)\n",
    "        self.llm = ChatOllama(model=llm_model_name)\n",
    "        self.reranker = Reranker(cross_encoder_model)\n",
    "\n",
    "        # Set up vector store\n",
    "        self.vector_store = VectorStore(self.embeddings_model)\n",
    "\n",
    "        # Process and index the PDF\n",
    "        self._process_document(pdf_path)\n",
    "\n",
    "    def _process_document(self, pdf_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Process the PDF document and populate the vector store.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF file\n",
    "        \"\"\"\n",
    "        extractor = TextExtractor(pdf_path)\n",
    "        chunks = extractor.process()\n",
    "        self.vector_store.add_documents(chunks)\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 10, rerank_top_k: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query with reranking.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query\n",
    "            top_k (int): Number of documents to retrieve initially\n",
    "            rerank_top_k (int): Number of documents to return after reranking\n",
    "\n",
    "        Returns:\n",
    "            List of relevant document chunks\n",
    "        \"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embeddings_model.embed_query(query)\n",
    "\n",
    "        # First-stage retrieval (vector similarity)\n",
    "        retrieved_docs = self.vector_store.similarity_search(\n",
    "            query_embedding, top_k=top_k\n",
    "        )\n",
    "        retrieved_chunks = [doc for doc, _ in retrieved_docs]\n",
    "\n",
    "        # Second-stage reranking\n",
    "        reranked_docs = self.reranker.rerank(query, retrieved_chunks)\n",
    "\n",
    "        # Return top reranked documents\n",
    "        return [doc for doc, _ in reranked_docs[:rerank_top_k]]\n",
    "\n",
    "    def generate_response(self, query: str, context: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response based on the query and retrieved context.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query\n",
    "            context (List[str]): List of relevant document chunks\n",
    "\n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        # Combine context into a single string\n",
    "        context_str = \"\\n\\n\".join(\n",
    "            [f\"Context {i + 1}:\\n{text}\" for i, text in enumerate(context)]\n",
    "        )\n",
    "\n",
    "        # Create prompt with context\n",
    "        prompt = f\"\"\"Answer the following question based on the provided context.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Context:\n",
    "        {context_str}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "\n",
    "        # Generate response\n",
    "        response = self.llm.invoke(prompt)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abf05d-a702-4516-a1e7-ff91bbf7e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./dataset/health supplements/1. dietary supplements - for whom.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84435bb9-2faa-4f7c-8a7a-8972a36998f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363804d-293e-4fb7-8c66-018016ff0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic of this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5697e-82cd-478f-9928-3e43a1aa917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = rag.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd13db-cac6-43b9-aa67-889e9da85a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retrieved {len(relevant_docs)} relevant chunks after reranking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bbc5d-df8f-4fe2-9791-b120724980ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag.generate_response(query, relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3eef8-9cf6-44e2-8244-f9cbda4be6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"=== Processing query: '{query}' ===\")\n",
    "\n",
    "response = rag.generate_response(query, relevant_docs)\n",
    "\n",
    "print(\"\\n=== Relevant Documents ===\")\n",
    "for i, doc in enumerate(relevant_docs[:3]):\n",
    "    print(f\"\\nDocument {i + 1} (first 200 chars):\")\n",
    "    print(doc[:200] + \"...\")\n",
    "\n",
    "print(\"\\n=== Generated Response ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10dc6a1-b3ae-4ba8-8a55-8b96744a7b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
