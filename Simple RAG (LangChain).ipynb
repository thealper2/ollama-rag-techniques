{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64b0bf-1a3d-410a-8a85-d9bdf785de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import fitz\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd63166-8dd5-4b99-90b5-18952268e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    A simple Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "     Attributes:\n",
    "         pdf_path (str): Path to the PDF file.\n",
    "         model_name (str): Name of the Ollama model to use\n",
    "         chunk_size (int): Size of text chunks\n",
    "         chunk_overlap (int): Overlap between chunks\n",
    "         embeddings (Embeddings): Embedding model\n",
    "         llm (Ollama): Language model for generation\n",
    "         vector_store (FAISS): Vector store for embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdf_path: str,\n",
    "        model_name: str = \"llama3.2:3b\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the SimpleRAG system.\n",
    "\n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            model_name: Name of the Ollama model\n",
    "            chunk_size: Size of text chunks\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        self.pdf_path = pdf_path\n",
    "        self.model_name = model_name\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.embedding = None\n",
    "        self.llm = None\n",
    "        self.vector_store = None\n",
    "\n",
    "    def setup_environment(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Ollama embeddings and LLM client.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize embedding model\n",
    "            self.embeddings = OllamaEmbeddings(model=self.model_name)\n",
    "\n",
    "            # Initialize LLM for generation\n",
    "            self.llm = ChatOllama(\n",
    "                base_url=\"http://localhost:11434\", model=self.model_name\n",
    "            )\n",
    "            print(\"Environment setup complete.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up environment: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_text_from_pdf(self) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from a PDF file using PyMuPDF.\n",
    "\n",
    "        Returns:\n",
    "            Extracted text as a single string\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If PDF doesn't exist\n",
    "            Exception: For other extraction errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with fitz.open(self.pdf_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "\n",
    "            print(f\"Successfully extracted text from {self.pdf_path}\")\n",
    "            return text\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: PDF file not found at {self.pdf_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {e}\")\n",
    "            raise\n",
    "\n",
    "    def chunk_text(self, text: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split text into smaller chunks with overlap.\n",
    "\n",
    "        Args:\n",
    "            text: The text to split\n",
    "\n",
    "        Returns:\n",
    "            List of Document objects containing text chunks\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize text splitter\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "                length_function=len,\n",
    "            )\n",
    "\n",
    "            # Create documents from text\n",
    "            documents = text_splitter.create_documents([text])\n",
    "            print(f\"Split text into {len(documents)} chunks\")\n",
    "            return documents\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error chunking text: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Create embeddings for text chunks and store them in a vector database.\n",
    "\n",
    "        Args:\n",
    "            documents: List of Document objects to embed\n",
    "\n",
    "        Raises:\n",
    "            Exception: If embedding creation fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.embeddings:\n",
    "                raise ValueError(\n",
    "                    \"Embeddings not initialized. Call setup_environment() first.\"\n",
    "                )\n",
    "\n",
    "            # Create vector store from documents\n",
    "            self.vector_store = FAISS.from_documents(documents, self.embeddings)\n",
    "            print(\"Successfully created embeddings and vector store\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_search(self, query: str, k: int = 3) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform semantic search on the stored embeddings.\n",
    "\n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return (default: 3)\n",
    "\n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "\n",
    "        Raises:\n",
    "            Exception: If search fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.vector_store:\n",
    "                raise ValueError(\n",
    "                    \"Vector store not initialized. Call create_embeddings() first.\"\n",
    "                )\n",
    "\n",
    "            # Perform similarity search\n",
    "            results = self.vector_store.similarity_search(query, k=k)\n",
    "            print(f\"Found {len(results)} relevant chunks for query: '{query}'\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error performing semantic search: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_response(self, query: str, context: List[Document]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using the LLM based on retrieved context.\n",
    "\n",
    "        Args:\n",
    "            query: User query\n",
    "            context: Retrieved relevant documents\n",
    "\n",
    "        Returns:\n",
    "            Generated response as a string\n",
    "\n",
    "        Raises:\n",
    "            Exception: If generation fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.llm:\n",
    "                raise ValueError(\"LLM not initialized. Call setup_environment() first.\")\n",
    "\n",
    "            # Combine context documents into a single string\n",
    "            context_str = \"\\n\\n\".join([doc.page_content for doc in context])\n",
    "\n",
    "            # Create prompt with context and query\n",
    "            prompt = f\"\"\"Answer the following question based on the provided context.\n",
    "                        If you don't know the answer, say \"I don't know\".\n",
    "                        \n",
    "                        Context:\n",
    "                        {context_str}\n",
    "                        \n",
    "                        Question: {query}\n",
    "                        Answer:\"\"\"\n",
    "\n",
    "            # Generate response\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return response.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_query(\n",
    "        self, query: str, expected_answer: Optional[str] = None\n",
    "    ) -> Tuple[str, Optional[float]]:\n",
    "        \"\"\"\n",
    "        Run a complete RAG pipeline for a query.\n",
    "\n",
    "        Args:\n",
    "            query: The user query\n",
    "            expected_answer: Optional expected answer for evaluation\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (generated_response, evaluation_score)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Perform semantic search\n",
    "            relevant_docs = self.semantic_search(query)\n",
    "\n",
    "            # Step 2: Generate response\n",
    "            response = self.generate_response(query, relevant_docs)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error running query: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adabd4c-71d2-48a6-a9bf-a4579e2823c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./dataset/health supplements/1. dietary supplements - for whom.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76488053-db5a-46e0-a26d-37c1063ee46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = SimpleRAG(pdf_path=pdf_path)\n",
    "rag.setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5882d5d-bbc1-427f-8d38-8d3e4feac1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = rag.extract_text_from_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a57290-aeff-4e10-ad21-d42cdd1f9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = rag.chunk_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df950638-c971-44fc-a158-27694cbfaa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba7cd4-fb5e-49a9-8192-f5c4c470c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic of this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689a1af-dcc7-43a4-b238-d424fd9d7213",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag.run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21310bd2-85e6-4127-ade9-786271f55a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Query ===\")\n",
    "print(query)\n",
    "\n",
    "print(\"\\n=== Response ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a1e3d-952c-4c90-8b7c-6a946b9da26e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
