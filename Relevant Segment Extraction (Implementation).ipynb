{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd231f6-8430-4bdf-80a8-18b61f27df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import fitz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4436a47-b195-404c-81da-f0154d143b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Handles PDF text extraction and chunking operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentProcessor with chunking parameters.\n",
    "\n",
    "        Args:\n",
    "            chunk_size (int): Size of each text chunk in characters (default: 1000)\n",
    "            chunk_overlap (int): Overlap between chunks in characters (default: 200)\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from a PDF file using PyMuPDF (fitz).\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF file\n",
    "\n",
    "        Returns:\n",
    "            Extracted text as a single string\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the PDF file doesn't exist\n",
    "            Exception: For other extraction errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "            return text\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"The file {pdf_path} was not found.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error extracting text from PDF: {str(e)}\")\n",
    "\n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into overlapping chunks.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to chunk\n",
    "\n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            chunks.append(text[start:end])\n",
    "            if end == len(text):\n",
    "                break\n",
    "            start = end - self.chunk_overlap\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07156be0-da47-47f5-af19-288650d9726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    A simple in-memory vector store for storing and searching embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the vector store with empty dictionaries.\"\"\"\n",
    "        self.embeddings = {}  # {chunk_id: embedding}\n",
    "        self.metadata = {}  # {chunk_id: metadata}\n",
    "        self.next_id = 0\n",
    "\n",
    "    def add_embedding(self, embedding: np.ndarray, metadata: Dict) -> int:\n",
    "        \"\"\"\n",
    "        Add an embedding to the vector store.\n",
    "\n",
    "        Args:\n",
    "            embedding (np.ndarray): The embedding vector to add\n",
    "            metadata (metadata): Associated metadata for the embedding\n",
    "\n",
    "        Returns:\n",
    "            The assigned ID for the embedding\n",
    "        \"\"\"\n",
    "        chunk_id = self.next_id\n",
    "        self.embeddings[chunk_id] = embedding\n",
    "        self.metadata[chunk_id] = metadata\n",
    "        self.next_id += 1\n",
    "        return chunk_id\n",
    "\n",
    "    def find_similar(\n",
    "        self, query_embedding: np.ndarray, top_k: int = 5\n",
    "    ) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Find the most similar embeddings to the query.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (np.ndarray): The embedding to compare against\n",
    "            top_k (int): Number of similar items to return (default: 5)\n",
    "\n",
    "        Returns:\n",
    "            List of tuples (chunk_id, similarity_score)\n",
    "        \"\"\"\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "\n",
    "        # Convert embeddings to matrix\n",
    "        ids = list(self.embeddings.keys())\n",
    "        embeddings_matrix = np.array([self.embeddings[id] for id in ids])\n",
    "\n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity(\n",
    "            query_embedding.reshape(1, -1), embeddings_matrix\n",
    "        )[0]\n",
    "\n",
    "        # Get top_k results\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(ids[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "    def get_text(self, chunk_id: int) -> str:\n",
    "        \"\"\"\n",
    "        Get the text associated with a chunk ID.\n",
    "\n",
    "        Args:\n",
    "            chunk_id (int): The ID of the chunk\n",
    "\n",
    "        Returns:\n",
    "            The text content of the chunk\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If the chunk ID doesn't exist\n",
    "        \"\"\"\n",
    "        return self.metadata[chunk_id][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57325566-151c-4b0c-93b3-11f636e0345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevantSegmentExtract:\n",
    "    \"\"\"\n",
    "    Implements the Relevant Segment Extraction (RSE) algorithm for RAG.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ollama_model: str = \"llama3.2:3b\"):\n",
    "        \"\"\"\n",
    "        Initialize the RSE core with Ollama models.\n",
    "\n",
    "        Args:\n",
    "            ollama_model (str): The name of the Ollama model to use\n",
    "        \"\"\"\n",
    "        self.embedding_model = OllamaEmbeddings(model=ollama_model)\n",
    "        self.llm = ChatOllama(model=ollama_model)\n",
    "        self.vector_store = VectorStore()\n",
    "\n",
    "    def generate_embeddings(self, chunks: List[str]) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of text chunks.\n",
    "\n",
    "        Args:\n",
    "            chunks (List[str]): List of text chunks to embed\n",
    "\n",
    "        Returns:\n",
    "            List of embedding vectors\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                embedding = self.embedding_model.embed_query(chunk)\n",
    "                embeddings.append(np.array(embedding))\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating embedding for chunk: {str(e)}\")\n",
    "                # Use zero vector as fallback\n",
    "                embeddings.append(np.zeros(4096))  # Assuming 4096-dim embeddings\n",
    "        return embeddings\n",
    "\n",
    "    def compute_chunk_values(self, chunks: List[str], query: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute relevance values for chunks based on a query.\n",
    "\n",
    "        Args:\n",
    "            chunks (List[str]): List of text chunks\n",
    "            query (str): The search query\n",
    "\n",
    "        Returns:\n",
    "            List of relevance scores for each chunk\n",
    "        \"\"\"\n",
    "        # Generate embeddings for chunks and query\n",
    "        chunk_embeddings = self.generate_embeddings(chunks)\n",
    "        query_embedding = np.array(self.embedding_model.embed_query(query))\n",
    "\n",
    "        # Calculate cosine similarity between query and each chunk\n",
    "        similarities = cosine_similarity(\n",
    "            query_embedding.reshape(1, -1), np.array(chunk_embeddings)\n",
    "        )[0]\n",
    "\n",
    "        return similarities.tolist()\n",
    "\n",
    "    def find_best_segments(\n",
    "        self,\n",
    "        chunks: List[str],\n",
    "        chunk_values: List[float],\n",
    "        top_k: int = 3,\n",
    "        min_value: float = 0.3,\n",
    "    ) -> List[Tuple[int, str, float]]:\n",
    "        \"\"\"\n",
    "        Identify the most relevant segments based on computed values.\n",
    "\n",
    "        Args:\n",
    "            chunks (List[str]): List of text chunks\n",
    "            chunk_values (List[float]): Computed relevance values for chunks\n",
    "            top_k (int): Number of segments to return (default: 3)\n",
    "            min_value (float): Minimum relevance score to consider (default: 0.3)\n",
    "\n",
    "        Returns:\n",
    "            List of tuples (index, chunk, value) for best segments\n",
    "        \"\"\"\n",
    "        # Pair chunks with their values and filter by minimum value\n",
    "        scored_chunks = [\n",
    "            (i, chunks[i], chunk_values[i])\n",
    "            for i in range(len(chunks))\n",
    "            if chunk_values[i] >= min_value\n",
    "        ]\n",
    "\n",
    "        # Sort by value in descending order\n",
    "        scored_chunks.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # Return top_k segments\n",
    "        return scored_chunks[:top_k]\n",
    "\n",
    "    def visualize_chunk_relevance(\n",
    "        self, chunk_values: List[float], title: str = \"Chunk Relevance Distribution\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Visualize the distribution of chunk relevance scores.\n",
    "\n",
    "        Args:\n",
    "            chunk_values (List[float]): List of relevance scores\n",
    "            title (str): Title for the plot\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(chunk_values, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Chunk Index\")\n",
    "        plt.ylabel(\"Relevance Score\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def process_document(\n",
    "        self, pdf_path: str, query: str\n",
    "    ) -> List[Tuple[int, str, float]]:\n",
    "        \"\"\"\n",
    "        Full document processing pipeline with RSE.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF document\n",
    "            query (str): The search query\n",
    "\n",
    "        Returns:\n",
    "            List of relevant segments (index, text, score)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Extract and chunk text\n",
    "            processor = DocumentProcessor()\n",
    "            text = processor.extract_text_from_pdf(pdf_path)\n",
    "            chunks = processor.chunk_text(text)\n",
    "\n",
    "            # Step 2: Compute chunk values\n",
    "            chunk_values = self.compute_chunk_values(chunks, query)\n",
    "\n",
    "            # Step 3: Visualize relevance distribution\n",
    "            self.visualize_chunk_relevance(chunk_values)\n",
    "\n",
    "            # Step 4: Find best segments\n",
    "            best_segments = self.find_best_segments(chunks, chunk_values)\n",
    "\n",
    "            # Step 5: Store chunks in vector store (for potential later use)\n",
    "            embeddings = self.generate_embeddings(chunks)\n",
    "            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                self.vector_store.add_embedding(\n",
    "                    embedding, {\"text\": chunk, \"index\": i, \"score\": chunk_values[i]}\n",
    "                )\n",
    "\n",
    "            return best_segments\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def generate_response_with_context(\n",
    "        self, query: str, context_segments: List[str]\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using the LLM with RSE-provided context.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query\n",
    "            context_segments (List[str]): List of relevant context segments\n",
    "\n",
    "        Returns:\n",
    "            The generated response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Combine context segments\n",
    "            context = \"\\n\\n\".join(\n",
    "                [f\"Context {i + 1}:\\n{seg}\" for i, seg in enumerate(context_segments)]\n",
    "            )\n",
    "\n",
    "            # Create prompt with context\n",
    "            prompt = f\"\"\"Answer the following question based on the provided context.\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {query}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "\n",
    "            # Generate response\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return response.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {str(e)}\")\n",
    "            return \"I couldn't generate a response due to an error.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec87389-8b32-452a-8a5b-8e0ac4e703bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rse = RelevantSegmentExtract(ollama_model=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ef7e3-8c22-46fa-85d1-b7d996b9d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to lose weight?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99bef8f-b081-4367-a294-de496c8e7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./dataset/health supplements/1. dietary supplements - for whom.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3c2b0-0e7d-4b2a-926d-16dd6f8195f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_segments = rse.process_document(pdf_path, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57356422-f266-4ad2-9877-61eb18c4da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRelevant Segments Found:\")\n",
    "for idx, segment, score in relevant_segments:\n",
    "    print(f\"\\nSegment {idx} (Score: {score:.2f}):\")\n",
    "    print(segment[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9a60c-39e6-4b32-ac82-47bb70f81a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if relevant_segments:\n",
    "    context_texts = [seg for _, seg, _ in relevant_segments]\n",
    "    response = rse.generate_response_with_context(query, context_texts)\n",
    "    print(\"\\nGenerated Response:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16130599-bb18-4dd6-ac98-1a86895e3194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
