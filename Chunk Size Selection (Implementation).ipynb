{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd67e7-a81e-40c5-9817-ac1977923f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from IPython.display import display, Markdown\n",
    "from rich.console import Console\n",
    "from rich.text import Text\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "from rich.syntax import Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef735786-b54c-424f-b24c-adfbaddbcdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtractor:\n",
    "    \"\"\"\n",
    "    A class to handle PDF text extraction using PyMuPDF.\n",
    "\n",
    "    Attributes:\n",
    "        file_path (str): Path to the PDF file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the text extractor with the file path.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the PDF file to extract text from\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def extract_text(self) -> str:\n",
    "        \"\"\"\n",
    "        Extract all text from the PDF file.\n",
    "\n",
    "        Returns:\n",
    "            Extracted text as a single string\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the PDF file doesn't exist\n",
    "            Exception: For other PDF reading errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            full_text = []\n",
    "            with fitz.open(self.file_path) as doc:\n",
    "                for page in doc:\n",
    "                    full_text.append(page.get_text())\n",
    "\n",
    "            return \"\\n\".join(full_text)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"The file {self.file_path} was not found.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred while reading the PDF: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc7c58-8948-4323-a680-5c3187b1d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextChunker:\n",
    "    \"\"\"\n",
    "    A class to handle semantic chunking of text with overlapping chunks.\n",
    "\n",
    "    Attributes:\n",
    "        text (str): The text to be chunked\n",
    "        chunk_size (int): Size of each chunk in tokens\n",
    "        overlap (int): Overlap between chunks in tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str, chunk_size: int = 256, overlap: int = 64):\n",
    "        \"\"\"\n",
    "        Initialize the text chunker with text and chunking parameters.\n",
    "\n",
    "        Args:\n",
    "            text: The text to be chunked\n",
    "            chunk_size: Desired size of each chunk in tokens\n",
    "            overlap: Desired overlap between chunks in tokens\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def tokenize_approximate(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Approximate tokenization by splitting on whitespace and punctuation.\n",
    "\n",
    "        Args:\n",
    "            text: Text to tokenize\n",
    "\n",
    "        Returns:\n",
    "            List of approximate tokens\n",
    "        \"\"\"\n",
    "        tokens = re.findall(r'\\w+|\\S', text)\n",
    "        return tokens\n",
    "\n",
    "    def chunk_text(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split the text into overlapping chunks based on the specified parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize_approximate(self.text)\n",
    "        chunks = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(tokens):\n",
    "            # Calculate end position for this chunk\n",
    "            end = min(i + self.chunk_size, len(tokens))\n",
    "\n",
    "            # Get the tokens for this chunk\n",
    "            chunk_tokens = tokens[i:end]\n",
    "            chunk_text = ' '.join(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "\n",
    "            # Move the starting position with overlap\n",
    "            i += (self.chunk_size - self.overlap)\n",
    "\n",
    "            # Ensure we don't get stuck in infinite loop with small chunks\n",
    "            if end == len(tokens):\n",
    "                break\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2767732-e6e2-4b09-bbc9-278899dfb041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"\n",
    "    A class to handle text embedding generation using Ollama.\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): Name of the Ollama model to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'llama3.2:3b'):\n",
    "        \"\"\"\n",
    "        Initialize the embedding generator with the model name.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the Ollama model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model = OllamaEmbeddings(model=model_name)\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of text strings.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): List of text strings to embed\n",
    "\n",
    "        Returns:\n",
    "            List of embedding vectors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.embedding_model.embed_documents(texts)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to generate embeddings: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75dc89-da84-4dcd-9008-1babb7e84301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:\n",
    "    \"\"\"\n",
    "    A class to perform semantic search on embedded text chunks.\n",
    "\n",
    "    Attributes:\n",
    "        chunks (List[str]): List of text chunks\n",
    "        embeddings (List[List[float]): Corresponding embeddings for chunks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chunks: List[str], embeddings: List[List[float]]):\n",
    "        \"\"\"\n",
    "        Initialize the semantic search with chunks and their embeddings.\n",
    "\n",
    "        Args:\n",
    "            chunks (List[str]): List of text chunks\n",
    "            embeddings (List[List[float]]): Corresponding embeddings for each chunk\n",
    "        \"\"\"\n",
    "        self.chunks = chunks\n",
    "        self.embeddings = np.array(embeddings)\n",
    "\n",
    "    def query(self, query_text: str, query_embedding: List[float], top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Perform a semantic search query and return top matching chunks.\n",
    "\n",
    "        Args:\n",
    "            query_text (str): The query text\n",
    "            query_embedding (List[float]): The embedding of the query\n",
    "            top_k (int): Number of top results to return\n",
    "\n",
    "        Returns:\n",
    "            List of tuples (chunk_text, similarity_score) sorted by score\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate cosine similarities\n",
    "            query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "            similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "\n",
    "            # Get top k results\n",
    "            top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "            results = [(self.chunks[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Semantic search failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860d7f8-a417-4e92-bd3b-b865c03fe6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseGenerator:\n",
    "    \"\"\"\n",
    "    A class to generate responses using retrieved chunks and Ollama.\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): Name of the Ollama model to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'llama3.2:3b'):\n",
    "        \"\"\"\n",
    "        Initialize the response generator with the model name.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the Ollama model to use for generation\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.llm = ChatOllama(model=model_name)\n",
    "\n",
    "    def generate_response(self, query: str, context_chunks: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response to a query using the provided context chunks.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query\n",
    "            context_chunks (List[str]): List of relevant context chunks\n",
    "\n",
    "        Returns:\n",
    "            Generated response text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Combine context results\n",
    "            context = \"\\n\\n\".join([f\"Context {i + 1}:\\n{chunk}\" for i, chunk in enumerate(context_chunks)])\n",
    "\n",
    "            # Create prompt\n",
    "            prompt = f\"\"\"You are a helpful assistant. Answer the user's question based on the provided context.\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {query}\n",
    "\n",
    "            Answer:\"\"\"\n",
    "\n",
    "            # Generate response\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return response.content\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to generate response: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36b987-57ef-4619-9ee9-e9ba79debf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseEvaluator:\n",
    "    \"\"\"\n",
    "    A class to evaluate generated responses based on faithfulness and relevancy.\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): Name of the Ollama model to use for evaluation\n",
    "        max_entries (int): Maximum number of retries for evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'llama3.2:3b', max_retries: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the response evaluator with the model name.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the Ollama model to use for evaluation\n",
    "            max_retries (int): Maximum attempts to get a valid evaluation\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.llm = ChatOllama(model=model_name)\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def parse_evaluation(self, eval_text: str) -> Optional[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Parse the evaluation text to extract scores.\n",
    "\n",
    "        Args:\n",
    "            eval_text (str): Raw evaluation text from LLM\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with scores if parsing succeeded, None otherwise\n",
    "        \"\"\"\n",
    "        # Try to find two numbers in the text\n",
    "        numbers = re.findall(r'\\b\\d+\\.?\\d*\\b', eval_text)\n",
    "        if len(numbers) >= 2:\n",
    "            try:\n",
    "                faithfulness = min(max(float(numbers[0]), 0.0), 1.0)\n",
    "                relevancy = min(max(float(numbers[1]), 0.0), 1.0)\n",
    "                return { 'faithfulness': faithfulness, 'relevancy': relevancy }\n",
    "\n",
    "            except (ValueError, TypeError):\n",
    "                return None\n",
    "\n",
    "        return None\n",
    "\n",
    "    def evaluate_response(self, query: str, context_chunks: List[str], response: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate a generated response on faithfulness and relevancy.\n",
    "        \n",
    "        Args:\n",
    "            query: The original user query\n",
    "            context_chunks: List of context chunks used\n",
    "            response: The generated response to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with 'faithfulness' and 'relevancy' scores (0-1)\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If evaluation fails after max retries\n",
    "        \"\"\"\n",
    "        # Combine context chunks\n",
    "        context = \"\\n\\n\".join([f\"Context {i+1}:\\n{chunk}\" for i, chunk in enumerate(context_chunks)])\n",
    "        \n",
    "        # Create more explicit evaluation prompt\n",
    "        prompt = f\"\"\"Please evaluate the response based on the following criteria:\n",
    "        1. Faithfulness (0-1): How accurately the response reflects the provided context. \n",
    "           - 1.0: Perfectly matches the context\n",
    "           - 0.5: Somewhat matches but has minor inaccuracies\n",
    "           - 0.0: Completely contradicts or invents information not in context\n",
    "        2. Relevancy (0-1): How well the response addresses the user's question.\n",
    "           - 1.0: Directly and completely answers the question\n",
    "           - 0.5: Partially answers or somewhat relevant\n",
    "           - 0.0: Completely irrelevant to the question\n",
    "        \n",
    "        Provide ONLY two numbers between 0 and 1 separated by a single space, representing:\n",
    "        faithfulness_score relevancy_score\n",
    "        \n",
    "        Example: \"0.8 0.9\"\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Response: {response}\n",
    "        \n",
    "        Evaluation scores:\"\"\"\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                eval_response = self.llm.invoke(prompt)\n",
    "                parsed = self.parse_evaluation(eval_response.content)\n",
    "                \n",
    "                if parsed:\n",
    "                    return parsed\n",
    "                else:\n",
    "                    print(f\"Retry {attempt + 1}: Could not parse evaluation, trying again...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Retry {attempt + 1}: Evaluation failed: {str(e)}\")\n",
    "        \n",
    "        # If we get here, all retries failed\n",
    "        print(\"Warning: Returning default scores after evaluation failure\")\n",
    "        return {'faithfulness': 0.5, 'relevancy': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b749881-7049-44c0-a8b9-9c0bd1b3d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    A complete RAG pipeline implementation with semantic chunking.\n",
    "    \n",
    "    Attributes:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_sizes (List[int]): List of chunk sizes to compare\n",
    "        overlap_ratio (float): Ratio of overlap between chunks (0-1)\n",
    "        embedding_model (str): Name of the embedding model\n",
    "        llm_model (str): Name of the LLM model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_path: str, \n",
    "                 chunk_sizes: List[int] = [128, 256, 512], \n",
    "                 overlap_ratio: float = 0.25,\n",
    "                 embedding_model: str = 'llama3.2:3b',\n",
    "                 llm_model: str = 'llama3.2:3b'):\n",
    "        \"\"\"\n",
    "        Initialize the RAG pipeline.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            chunk_sizes: List of chunk sizes to compare\n",
    "            overlap_ratio: Ratio of overlap between chunks\n",
    "            embedding_model: Name of the embedding model\n",
    "            llm_model: Name of the LLM model\n",
    "        \"\"\"\n",
    "        self.pdf_path = pdf_path\n",
    "        self.chunk_sizes = chunk_sizes\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "        \n",
    "        # Initialize components\n",
    "        self.text_extractor = TextExtractor(pdf_path)\n",
    "        self.embedding_generator = EmbeddingGenerator(embedding_model)\n",
    "        self.response_generator = ResponseGenerator(llm_model)\n",
    "        self.response_evaluator = ResponseEvaluator(llm_model)\n",
    "        \n",
    "        # Storage for different chunking strategies\n",
    "        self.chunk_data = {}\n",
    "    \n",
    "    def process_document(self):\n",
    "        \"\"\"\n",
    "        Process the document by extracting text and creating chunks with different sizes.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract text from PDF\n",
    "            text = self.text_extractor.extract_text()\n",
    "            \n",
    "            # Create chunks for each chunk size\n",
    "            for size in self.chunk_sizes:\n",
    "                overlap = int(size * self.overlap_ratio)\n",
    "                chunker = TextChunker(text, chunk_size=size, overlap=overlap)\n",
    "                chunks = chunker.chunk_text()\n",
    "                embeddings = self.embedding_generator.generate_embeddings(chunks)\n",
    "                \n",
    "                self.chunk_data[size] = {\n",
    "                    'chunks': chunks,\n",
    "                    'embeddings': embeddings,\n",
    "                    'search': SemanticSearch(chunks, embeddings)\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Document processing failed: {str(e)}\")\n",
    "    \n",
    "    def query(self, query_text: str, top_k: int = 3) -> Dict[int, Dict[str, Union[str, Dict[str, float]]]]:\n",
    "        \"\"\"\n",
    "        Execute a query against all chunking strategies and return results.\n",
    "        \n",
    "        Args:\n",
    "            query_text: The query text\n",
    "            top_k: Number of chunks to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results for each chunk size, containing:\n",
    "            - retrieved_chunks: List of (chunk, similarity) tuples\n",
    "            - response: Generated response\n",
    "            - evaluation: Faithfulness and relevancy scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            query_embedding = self.embedding_generator.generate_embeddings([query_text])[0]\n",
    "            \n",
    "            results = {}\n",
    "            \n",
    "            for size, data in self.chunk_data.items():\n",
    "                # Perform semantic search\n",
    "                retrieved_chunks = data['search'].query(query_text, query_embedding, top_k)\n",
    "                chunk_texts = [chunk for chunk, _ in retrieved_chunks]\n",
    "                \n",
    "                # Generate response\n",
    "                response = self.response_generator.generate_response(query_text, chunk_texts)\n",
    "                \n",
    "                # Evaluate response\n",
    "                evaluation = self.response_evaluator.evaluate_response(query_text, chunk_texts, response)\n",
    "                \n",
    "                results[size] = {\n",
    "                    'retrieved_chunks': retrieved_chunks,\n",
    "                    'response': response,\n",
    "                    'evaluation': evaluation\n",
    "                }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Query failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352d508-2b6b-4c8f-b22a-57cfea1d8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./dataset/health supplements/1. dietary supplements - for whom.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef7cfa-3e46-4d43-a271-d0caa957b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d848b-8e79-4c85-9921-ef97b64cec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAGPipeline(pdf_path, chunk_sizes=[128, 256, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb57ffe-2527-4642-b0af-8c7c2e347d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.process_document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da6ff5-509b-426e-b62c-8f1009af13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main findings of this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae18817-13e8-4cd6-93f3-53c6f449d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rag.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689c568-61eb-4647-811a-f30fa08aa5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size, result in results.items():\n",
    "    title = Text(f\"Results for Chunk Size: {size}\", style=\"bold blue\")\n",
    "    \n",
    "    faithfulness = result['evaluation']['faithfulness']\n",
    "    relevancy = result['evaluation']['relevancy']\n",
    "    \n",
    "    faith_style = \"green\" if faithfulness > 0.7 else \"yellow\" if faithfulness > 0.4 else \"red\"\n",
    "    rel_style = \"green\" if relevancy > 0.7 else \"yellow\" if relevancy > 0.4 else \"red\"\n",
    "    \n",
    "    eval_text = Text()\n",
    "    eval_text.append(\"Evaluation - \", style=\"bold\")\n",
    "    eval_text.append(f\"Faithfulness: {faithfulness:.2f}\", style=faith_style)\n",
    "    eval_text.append(\", \", style=\"bold\")\n",
    "    eval_text.append(f\"Relevancy: {relevancy:.2f}\", style=rel_style)\n",
    "    \n",
    "    response = Syntax(result['response'], \"python\", theme=\"monokai\", line_numbers=False)\n",
    "    \n",
    "    chunks_table = Table(title=\"Top Retrieved Chunks\", show_header=True, header_style=\"bold magenta\")\n",
    "    chunks_table.add_column(\"Chunk #\", style=\"cyan\", no_wrap=True)\n",
    "    chunks_table.add_column(\"Score\", style=\"green\")\n",
    "    chunks_table.add_column(\"Content Preview\")\n",
    "    \n",
    "    for i, (chunk, score) in enumerate(result['retrieved_chunks']):\n",
    "        score_style = \"green\" if score > 0.7 else \"yellow\" if score > 0.4 else \"red\"\n",
    "        chunks_table.add_row(\n",
    "            str(i+1),\n",
    "            f\"[{score_style}]{score:.2f}[/]\",\n",
    "            chunk[:100] + \"...\"\n",
    "        )\n",
    "    \n",
    "    console.print(Panel.fit(title))\n",
    "    console.print(eval_text)\n",
    "    console.print(\"\\n[bold]Response:[/]\")\n",
    "    console.print(response)\n",
    "    console.print(chunks_table)\n",
    "    console.print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514408f-5d48-474e-979f-d4ce06146f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c9360a-a66e-482a-9635-53b894623ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac217b6-05c1-4f47-942c-9eea1d6070f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
