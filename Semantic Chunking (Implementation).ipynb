{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94dc828-59ae-4a50-8f2c-d6488beff239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from statistics import mean, stdev\n",
    "from typing import List, Dict, Tuple, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01ffb6-8b4e-475d-8fd3-fdc23de09bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticChunking:\n",
    "    \"\"\"\n",
    "    A class to implement semantic chunking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ollama_model: str = \"llama3.2:3b\"):\n",
    "        \"\"\"\n",
    "        Initialize the SemanticChunking with an Ollama model.\n",
    "        \n",
    "        Args:\n",
    "            ollama_model (str): The name of the Ollama model to use for chat and embeddings.\n",
    "        \"\"\"\n",
    "        self.ollama_model = ollama_model\n",
    "        self.embedding_model = OllamaEmbeddings(model=ollama_model)\n",
    "        self.chat_model = ChatOllama(model=ollama_model)\n",
    "        self.sentences: List[str] = []\n",
    "        self.sentence_embeddings: np.ndarray = None\n",
    "        self.similarity_diffs: List[float] = []\n",
    "        self.chunks: List[str] = []\n",
    "        self.chunk_embeddings: np.ndarray = None\n",
    "        \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from a PDF file using PyMuPDF.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF file.\n",
    "            \n",
    "        Returns:\n",
    "            str: Extracted text from the PDF.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the PDF file doesn't exist.\n",
    "            Exception: For other extraction errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"The PDF file at {pdf_path} was not found.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error extracting text from PDF: {str(e)}\")\n",
    "    \n",
    "    def split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into sentences using a simple regex pattern.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to split.\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of sentences.\n",
    "        \"\"\"\n",
    "        # This is a simple sentence splitter - you might want to use a more sophisticated one\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', text)\n",
    "        # Remove empty strings and strip whitespace\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        return sentences\n",
    "    \n",
    "    def generate_sentence_embeddings(self, sentences: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for each sentence using Ollama embeddings.\n",
    "        \n",
    "        Args:\n",
    "            sentences (List[str]): List of sentences to embed.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Array of sentence embeddings (n_sentences x embedding_dim).\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding_model.embed_documents(sentences)\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def calculate_similarity_differences(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity differences between consecutive sentences.\n",
    "        \n",
    "        Returns:\n",
    "            List[float]: List of similarity differences between consecutive sentences.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If sentence embeddings haven't been generated yet.\n",
    "        \"\"\"\n",
    "        if self.sentence_embeddings is None:\n",
    "            raise ValueError(\"Sentence embeddings must be generated first.\")\n",
    "            \n",
    "        similarity_diffs = []\n",
    "        for i in range(len(self.sentence_embeddings) - 1):\n",
    "            # Calculate cosine similarity between current and next sentence\n",
    "            sim = cosine_similarity(\n",
    "                self.sentence_embeddings[i].reshape(1, -1),\n",
    "                self.sentence_embeddings[i+1].reshape(1, -1)\n",
    "            )[0][0]\n",
    "            similarity_diffs.append(1 - sim)  # Using difference for breakpoint detection\n",
    "            \n",
    "        return similarity_diffs\n",
    "    \n",
    "    def find_breakpoints(self, method: str = \"percentile\", **kwargs) -> List[int]:\n",
    "        \"\"\"\n",
    "        Find breakpoints between chunks using different methods.\n",
    "        \n",
    "        Args:\n",
    "            method (str): Method to use for finding breakpoints. Options:\n",
    "                - \"percentile\": Use a percentile threshold\n",
    "                - \"standard_deviation\": Use mean + n*std_dev\n",
    "                - \"interquartile\": Use IQR method\n",
    "            **kwargs: Additional method-specific parameters.\n",
    "            \n",
    "        Returns:\n",
    "            List[int]: Indices of breakpoints between sentences.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If an invalid method is specified or similarity diffs not calculated.\n",
    "        \"\"\"\n",
    "        if not self.similarity_diffs:\n",
    "            raise ValueError(\"Similarity differences must be calculated first.\")\n",
    "            \n",
    "        diffs = self.similarity_diffs\n",
    "        \n",
    "        if method == \"percentile\":\n",
    "            percentile = kwargs.get('percentile', 95)\n",
    "            threshold = np.percentile(diffs, percentile)\n",
    "            breakpoints = [i for i, diff in enumerate(diffs) if diff > threshold]\n",
    "            \n",
    "        elif method == \"standard_deviation\":\n",
    "            n_std = kwargs.get('n_std', 1.5)\n",
    "            mean_diff = mean(diffs)\n",
    "            std_diff = stdev(diffs) if len(diffs) > 1 else 0\n",
    "            threshold = mean_diff + n_std * std_diff\n",
    "            breakpoints = [i for i, diff in enumerate(diffs) if diff > threshold]\n",
    "            \n",
    "        elif method == \"interquartile\":\n",
    "            q1 = np.percentile(diffs, 25)\n",
    "            q3 = np.percentile(diffs, 75)\n",
    "            iqr = q3 - q1\n",
    "            threshold = q3 + 1.5 * iqr\n",
    "            breakpoints = [i for i, diff in enumerate(diffs) if diff > threshold]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid method: {method}. Choose from 'percentile', 'standard_deviation', or 'interquartile'.\")\n",
    "            \n",
    "        return breakpoints\n",
    "    \n",
    "    def split_into_semantic_chunks(self, breakpoints: List[int]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split sentences into chunks based on breakpoints.\n",
    "        \n",
    "        Args:\n",
    "            breakpoints (List[int]): Indices where chunks should be split.\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of text chunks.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If sentences haven't been extracted yet.\n",
    "        \"\"\"\n",
    "        if not self.sentences:\n",
    "            raise ValueError(\"Sentences must be extracted first.\")\n",
    "            \n",
    "        # Add start and end points\n",
    "        breakpoints = sorted(breakpoints)\n",
    "        breakpoints = [0] + [bp + 1 for bp in breakpoints] + [len(self.sentences)]\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i+1]\n",
    "            chunk = ' '.join(self.sentences[start:end])\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    def generate_chunk_embeddings(self, chunks: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for each chunk.\n",
    "        \n",
    "        Args:\n",
    "            chunks (List[str]): List of text chunks to embed.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Array of chunk embeddings.\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding_model.embed_documents(chunks)\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def semantic_search(self, query: str, top_k: int = 3) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"\n",
    "        Perform semantic search to find the most relevant chunks to a query.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The search query.\n",
    "            top_k (int): Number of top results to return.\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[int, float, str]]: List of (index, similarity_score, chunk) tuples.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If chunks or chunk embeddings haven't been generated.\n",
    "        \"\"\"\n",
    "        if not self.chunks or self.chunk_embeddings is None:\n",
    "            raise ValueError(\"Chunks and chunk embeddings must be generated first.\")\n",
    "            \n",
    "        # Embed the query\n",
    "        query_embedding = np.array(self.embedding_model.embed_query(query))\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity(\n",
    "            query_embedding.reshape(1, -1),\n",
    "            self.chunk_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # Get top_k results\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        results = [(i, similarities[i], self.chunks[i]) for i in top_indices]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_response(self, query: str, context_chunks: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response to a query using the provided context chunks.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            context_chunks (List[str]): Relevant context chunks to use for generation.\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        # Combine context chunks into a single context\n",
    "        context = \"\\n\\n\".join(context_chunks)\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = f\"\"\"Use the following context to answer the question at the end. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        # Generate the response\n",
    "        response = self.chat_model.invoke(prompt)\n",
    "        return response.content if hasattr(response, 'content') else str(response)\n",
    "    \n",
    "    def process_document(self, pdf_path: str, chunk_method: str = \"percentile\", **chunk_kwargs):\n",
    "        \"\"\"\n",
    "        Process a document through the entire pipeline.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF file.\n",
    "            chunk_method (str): Method to use for semantic chunking.\n",
    "            **chunk_kwargs: Additional arguments for the chunking method.\n",
    "        \"\"\"\n",
    "        # Step 1: Extract text from PDF\n",
    "        text = self.extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        # Step 2: Split into sentences\n",
    "        self.sentences = self.split_into_sentences(text)\n",
    "        \n",
    "        # Step 3: Generate sentence embeddings\n",
    "        self.sentence_embeddings = self.generate_sentence_embeddings(self.sentences)\n",
    "        \n",
    "        # Step 4: Calculate similarity differences\n",
    "        self.similarity_diffs = self.calculate_similarity_differences()\n",
    "        \n",
    "        # Step 5: Find breakpoints\n",
    "        breakpoints = self.find_breakpoints(method=chunk_method, **chunk_kwargs)\n",
    "        \n",
    "        # Step 6: Split into semantic chunks\n",
    "        self.chunks = self.split_into_semantic_chunks(breakpoints)\n",
    "        \n",
    "        # Step 7: Generate chunk embeddings\n",
    "        self.chunk_embeddings = self.generate_chunk_embeddings(self.chunks)\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Query the system with a question and get a response.\n",
    "        \n",
    "        Args:\n",
    "            question (str): The question to ask.\n",
    "            top_k (int): Number of chunks to retrieve for context.\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        # Step 1: Semantic search to find relevant chunks\n",
    "        relevant_chunks = self.semantic_search(question, top_k=top_k)\n",
    "        context_chunks = [chunk for _, _, chunk in relevant_chunks]\n",
    "        \n",
    "        # Step 2: Generate response using the context\n",
    "        response = self.generate_response(question, context_chunks)\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56614f5-bdc3-4696-823a-83acdf29c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./dataset/health supplements/1. dietary supplements - for whom.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebede388-5ce6-4b39-a860-e99ba817802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = SemanticChunking(ollama_model=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20197749-e791-4f01-a0f5-76f28790b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.process_document(pdf_path, chunk_method=\"percentile\", percentile=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d9e18-d1e5-4354-a74a-069ab04f3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main findings of this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26752ed-3187-4cc2-96db-b99d3a2a12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55fae5-9d33-45bd-a285-948cffaf9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Response to '{query}':\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765a65b-3576-42db-9d82-3a825ad0445e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
