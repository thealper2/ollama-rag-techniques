{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2805146-dd76-430c-845b-1b0c549c184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import fitz\n",
    "import numpy as np\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from numpy.linalg import norm\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f622c4-642d-49c2-b713-76a77dd8388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(BaseModel):\n",
    "    \"\"\"A document with text content and metadata.\"\"\"\n",
    "\n",
    "    page_content: str\n",
    "    metadata: Dict[str, Any] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f126665-628d-4c75-a791-fbee99c78559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalResult(BaseModel):\n",
    "    \"\"\"Result of a retrieval operation containing documents and scores.\"\"\"\n",
    "\n",
    "    documents: List[Document]\n",
    "    scores: List[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5d222-3785-4e58-b011-54920ab75cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationOutput(BaseModel):\n",
    "    \"\"\"Output of the generation step with the response and reflection.\"\"\"\n",
    "\n",
    "    response: str\n",
    "    reflection: str\n",
    "    relevant_documents: List[Document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba8c0f-1335-4eda-9608-3afac66b22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfRAG:\n",
    "    \"\"\"Implementation of Self-RAG (Retrieval-Augmented Generation with Self-Reflection).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model: str = \"llama3.2:3b\",\n",
    "        embedding_model: str = \"llama3.2:3b\",\n",
    "        retrieval_top_k: int = 3,\n",
    "        reflection_threshold: float = 0.7,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Self-RAG system.\n",
    "\n",
    "        Args:\n",
    "            llm_model (str): Name of the Ollama LLM model to use\n",
    "            embedding_model (str): Name of the Ollama embedding model to use\n",
    "            retrieval_top_k (int): Number of documents to retrieve\n",
    "            reflection_threshold (float): Confidence threshold for triggering self-reflection\n",
    "        \"\"\"\n",
    "        self.llm = ChatOllama(model=llm_model, temperature=0.3)\n",
    "        self.embeddings = OllamaEmbeddings(model=embedding_model)\n",
    "        self.retrieval_top_k = retrieval_top_k\n",
    "        self.reflection_threshold = reflection_threshold\n",
    "        self.document_store: List[Document] = []\n",
    "\n",
    "    def add_documents(self, documents: List[Document]) -> None:\n",
    "        \"\"\"Add documents to the document store.\"\"\"\n",
    "        self.document_store.extend(documents)\n",
    "        print(\n",
    "            f\"Added {len(documents)} documents to the store. Total documents: {len(self.document_store)}\"\n",
    "        )\n",
    "\n",
    "    def load_pdf_documents(\n",
    "        self, file_path: str, metadata: Optional[Dict] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load and extract text from a PDF file using Fitz (PyMuPDF).\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the PDF file\n",
    "            metadata (Optional[Dict]): Optional metadata to attach to all documents from this file\n",
    "\n",
    "        Returns:\n",
    "            List of Document objects extracted from the PDF\n",
    "        \"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "\n",
    "        documents = []\n",
    "        try:\n",
    "            # Open the PDF file\n",
    "            with fitz.open(file_path) as pdf_file:\n",
    "                # Add file-specific metadata\n",
    "                file_metadata = {\n",
    "                    \"source\": file_path,\n",
    "                    \"total_pages\": len(pdf_file),\n",
    "                    **metadata,\n",
    "                }\n",
    "\n",
    "                # Extract text from each page\n",
    "                for page_num, page in enumerate(pdf_file):\n",
    "                    text = page.get_text()\n",
    "                    if text.strip():  # Only add pages with content\n",
    "                        page_metadata = {\"page_number\": page_num + 1, **file_metadata}\n",
    "                        documents.append(\n",
    "                            Document(page_content=text, metadata=page_metadata)\n",
    "                        )\n",
    "\n",
    "            print(f\"Loaded {len(documents)} pages from PDF: {file_path}\")\n",
    "            return documents\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading PDF file {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embeddings for a given text.\"\"\"\n",
    "        try:\n",
    "            return self.embeddings.embed_query(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "    def cosine_similarity(self, vec_a: List[float], vec_b: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "        return np.dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))\n",
    "\n",
    "    def retrieve_documents(self, query: str) -> RetrievalResult:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents based on the query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The input query to retrieve documents for\n",
    "\n",
    "        Returns:\n",
    "            RetrievalResult containing documents and their similarity scores\n",
    "        \"\"\"\n",
    "        if not self.document_store:\n",
    "            print(\"Document store is empty. No documents to retrieve.\")\n",
    "            return RetrievalResult(documents=[], scores=[])\n",
    "\n",
    "        try:\n",
    "            # Embed the query\n",
    "            query_embedding = self.embed_text(query)\n",
    "\n",
    "            # Calculate similarities with all documents\n",
    "            similarities = []\n",
    "            for doc in self.document_store:\n",
    "                doc_embedding = self.embed_text(doc.page_content)\n",
    "                similarity = self.cosine_similarity(query_embedding, doc_embedding)\n",
    "                similarities.append(similarity)\n",
    "\n",
    "            # Get top-k documents\n",
    "            top_indices = np.argsort(similarities)[-self.retrieval_top_k :][::-1]\n",
    "            top_documents = [self.document_store[i] for i in top_indices]\n",
    "            top_scores = [similarities[i] for i in top_indices]\n",
    "\n",
    "            return RetrievalResult(documents=top_documents, scores=top_scores)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during document retrieval: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_response(\n",
    "        self, query: str, context_documents: List[Document]\n",
    "    ) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Generate a response based on the query and context documents.\n",
    "\n",
    "        Args:\n",
    "            query (str): The input query\n",
    "            context_documents (List[Document]): Relevant documents to use as context\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (generated_response, confidence_score)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare context from documents\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in context_documents])\n",
    "\n",
    "            # Create prompt with context\n",
    "            prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "            If you don't know the answer, say you don't know. Be truthful and accurate.\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {query}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "\n",
    "            # Generate response\n",
    "            response = self.llm.invoke(prompt)\n",
    "            response_text = response.content\n",
    "\n",
    "            # Generate confidence score (simple implementation - could be enhanced)\n",
    "            confidence_prompt = f\"\"\"On a scale from 0 to 1, how confident are you that the following answer is correct?\n",
    "            Answer: {response_text}\n",
    "            \n",
    "            Provide only the numerical confidence score between 0 and 1:\"\"\"\n",
    "\n",
    "            confidence_response = self.llm.invoke(confidence_prompt)\n",
    "\n",
    "            try:\n",
    "                confidence = float(confidence_response.content.strip())\n",
    "            except ValueError:\n",
    "                print(\"Could not parse confidence score, defaulting to 0.5\")\n",
    "                confidence = 0.5\n",
    "\n",
    "            return response_text, confidence\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during response generation: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_reflection(\n",
    "        self, query: str, response: str, documents: List[Document]\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate a self-reflection on the quality of the response.\n",
    "\n",
    "        Args:\n",
    "            query (str): The original query\n",
    "            response (str): The generated response\n",
    "            documents (List[Document]): Documents used for the response\n",
    "\n",
    "        Returns:\n",
    "            Reflection text analyzing the response quality\n",
    "        \"\"\"\n",
    "        try:\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "            reflection_prompt = f\"\"\"Analyze the following QA interaction and provide a critical reflection:\n",
    "            \n",
    "            Question: {query}\n",
    "            \n",
    "            Context used:\n",
    "            {context}\n",
    "            \n",
    "            Answer provided:\n",
    "            {response}\n",
    "            \n",
    "            Reflection (consider accuracy, completeness, relevance to context, and potential improvements):\"\"\"\n",
    "\n",
    "            reflection_response = self.llm.invoke(reflection_prompt)\n",
    "            return reflection_response.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reflection generation: {e}\")\n",
    "            return \"Error generating reflection.\"\n",
    "\n",
    "    def __call__(self, query: str) -> GenerationOutput:\n",
    "        \"\"\"\n",
    "        Execute the full Self-RAG pipeline: retrieve, generate, reflect.\n",
    "\n",
    "        Args:\n",
    "            query (str): The input query\n",
    "\n",
    "        Returns:\n",
    "            GenerationOutput containing response, reflection, and relevant documents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Retrieve relevant documents\n",
    "            retrieval_result = self.retrieve_documents(query)\n",
    "            print(\n",
    "                f\"Retrieved {len(retrieval_result.documents)} documents for query: {query}\"\n",
    "            )\n",
    "\n",
    "            # Step 2: Generate initial response\n",
    "            response, confidence = self.generate_response(\n",
    "                query, retrieval_result.documents\n",
    "            )\n",
    "            print(f\"Generated response with confidence: {confidence:.2f}\")\n",
    "\n",
    "            # Step 3: Generate reflection if confidence is below threshold\n",
    "            reflection = \"\"\n",
    "            if confidence < self.reflection_threshold:\n",
    "                reflection = self.generate_reflection(\n",
    "                    query, response, retrieval_result.documents\n",
    "                )\n",
    "                print(\"Generated self-reflection due to low confidence\")\n",
    "\n",
    "            return GenerationOutput(\n",
    "                response=response,\n",
    "                reflection=reflection,\n",
    "                relevant_documents=retrieval_result.documents,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Self-RAG pipeline: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40caaeee-ad92-4657-9c1d-dd9e1ecf40fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = SelfRAG(llm_model=\"llama3.2:3b\", embedding_model=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67948888-f134-4177-ac92-b15c178bc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs = rag.load_pdf_documents(\n",
    "    file_path=\"./dataset/health supplements/1. dietary supplements - for whom.pdf\",\n",
    "    metadata={\"document_type\": \"research_paper\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1b697-61d8-4656-8bbb-27c5ddf2310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.add_documents(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3986e0-d458-455c-9897-6b31464a57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to lose weight?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201a9a1-c065-4dbf-b0fa-7f08fa38ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa979b-9956-474b-9667-d511b2edf9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Response ===\")\n",
    "print(result.response)\n",
    "\n",
    "if result.reflection:\n",
    "    print(\"\\n=== Reflection ===\")\n",
    "    print(result.reflection)\n",
    "\n",
    "print(\"\\n=== Relevant Documents ===\")\n",
    "for doc in result.relevant_documents:\n",
    "    print(\n",
    "        f\"- {doc.page_content[:100]}... (Source: {doc.metadata.get('source', 'unknown')})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b1026-75ee-4a81-bdcf-25c7433eaf1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
